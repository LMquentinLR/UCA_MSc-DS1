{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE 4 - Basic Algrebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*September 30th, 2020*\n",
    "\n",
    "**Teacher**: Marco Corneli\n",
    "\n",
    "*Contacts on personal website:* https://math.unice.fr/~mcorneli/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Complex Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $y \\in \\mathbb{C}$, $y = a + i . b$, $\\forall a, b \\in \\mathbb{R}$, $i^2 = -1$\n",
    "\n",
    "2. $\\bar{y}$ = conjugate, $\\bar{y} = a - i.b$\n",
    "\n",
    "3. $y.\\bar{y} = a^2 + b^2$\n",
    "\n",
    "4. $\\bar{x.y} = \\bar{x}.\\bar{y}$\n",
    "\n",
    "5. $y = \\bar{y} \\Leftrightarrow y \\in \\mathbb{R}$ if and only if $b = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_1 \\neq \\lambda_2 \\Rightarrow$ $<v_1, v_2>$ $= 0$ for a symmetric square matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, PCA uses 1 to 2 eigenvectors to project data into the $\\mathbb{R}^2$ vector space. Usually those eigenvectors have the highest magnitude (i.e. eigenvalues).\n",
    "\n",
    "1. Compute the covariance matrix of dimension $\\mathbb{N}$ x $\\mathbb{N}$\n",
    "\n",
    "2. Compute the eigenvalues and eigenvectors\n",
    "\n",
    "3. Take 2 eigenvectors corresponding to the heighest eigenvalues\n",
    "\n",
    "4. Create a dimension $\\mathbb{N}$x$2$ matrix with the selected eigenvectors as columns\n",
    "\n",
    "5. Multiply the original vectors with the created matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_A:\\mathbb{R}^N\\rightarrow\\mathbb{R}$, $\\forall \\mathbb{R}^N$, $f_A(v) = \\sum_{i=1}^N\\sum_{j=1}^NA_{i,j}.v_i.v_j$ with $A\\in\\mathbb{R}^{NxN}$ is called a quadratic form.\n",
    "\n",
    "> $v^T.A.v$\n",
    "\n",
    "1. $f_A(.)$ is positive (negative) definite if $f_A(v) \\ge 0$ $(\\le 0)$ and $F-A(v)=0$ iff $v=O_{\\mathbb{R}^N}$\n",
    "\n",
    "2. $f_A(.)$ is positive (negative) definite if $f_A(v) \\ge 0$ $(\\le 0)$ and $\\exists v \\ne O_N$ such that $f_A(v)=0$\n",
    "\n",
    "3. $f_A(.)$ is positive (negative) definite if and only if the eigenvalues of its associated matrix A are strictly positive (negative)\n",
    "\n",
    "4. $f_A(.)$ is semi-positive (negative) definite if and only if the eigenvalues of its associated matrix A are positive (negative) or null\n",
    "\n",
    "5. If none of the conditions above are met, f is not definite\n",
    "\n",
    "### Proof of 3 and 4.\n",
    "\n",
    "$\\forall v \\in \\mathbb{R}^N$, where $Q$ is the maxtrix which columns are the eigenvectors (norm one) of $A$ and $\\Lambda$ the diagonal matrix which non-null entries are the eigenvalues of $A$. It holds:\n",
    "\n",
    "> $v^T.A.v = v^T.Q.\\Lambda.Q^T.v$\n",
    "\n",
    "If we substitude $z = Q^T.v$ in the above equation we get:\n",
    "\n",
    "> $z^T.A.z = \\sum_{i=1}^N\\sum_{j=1}^N\\Lambda_{i,j}.z_i.z_j$\n",
    "> \n",
    "> $=\\sum_{i=1}^N\\Lambda_{i,i}.z_i.z_i = \\sum_{i=1}^N\\lambda_i.z_i^2$\n",
    "\n",
    "with $v\\ne O_v$ then $z = v^T.Q = Q^T.v\\ne 0$ hence $ker(Q)=\\{O_v\\}$\n",
    "\n",
    "Thus:\n",
    "\n",
    "> if $\\lambda_i, ..., \\lambda_n > 0 \\Rightarrow \\sum_{i=1}^N\\lambda_i.z_i^2 > 0$ and this proves the proposition.\n",
    "> \n",
    "> Vice-Versa, if $\\forall v \\ne 0$, $f_A(v) > 0$, we can choose $v$ such that $z = (0,...,1_i,...0) \\Rightarrow \\sum_{i=1}^N\\lambda_i.z_i^2 = \\lambda_i \\Rightarrow 0 < f_A(v)$\n",
    "> \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element of multivariate real analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a [not necessarily linear] map $f:\\mathbb{R}^N \\rightarrow \\mathbb{R}$, with N = 2 for simplicity of demonstration.\n",
    "\n",
    "> $f:\\mathbb{R}^2 \\rightarrow \\mathbb{R}$\n",
    "> \n",
    "> $(x,y) \\rightarrow f(x,y)$\n",
    "\n",
    "### Partial Derivatives\n",
    "\n",
    "> $\\frac{\\delta f}{\\delta x}(x,y) = f_X(x,y)= \\lim_{h\\to0}\\frac{f(x+h,y)-f(x,y)}{h}$\n",
    "> \n",
    "> $\\frac{\\delta f}{\\delta y}(x,y) = f_X(x,y)= \\lim_{h\\to0}\\frac{f(x,y+h)-f(x,y)}{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "The gradient of a multivariate function is the column vector formed with, for each row, its partial derivatives.\n",
    "\n",
    "Example: $f(x,y) = e^{x^2.y}$ has the following gradient:\n",
    "\n",
    "> $\\nabla_{f(x,y)} = \\begin{pmatrix} 2.x.y.e^{x^2.y} \\\\ x^2.e^{x^2.y} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional Derivatives\n",
    "\n",
    "A direction is a vector $v$ in $\\mathbb{R}^N$, $||v||=1$ such that $\\forall t\\in\\mathbb{R}$, $x+t.v$ is a straight line passing through $x$ in the direction $v$.\n",
    "\n",
    "> $g: \\mathbb{R}\\rightarrow\\mathbb{R}$, $g(t)=(f.x_0+t.v)$ for $x_0 \\in \\mathbb{R}$\n",
    ">\n",
    "> $\\frac{\\delta f}{\\delta x}(x_0) = g'(0) = \\lim_{t\\to0}\\frac{g(t)-g(0)}{t} = \\lim_{t\\to0}\\frac{f(x_0+t.v)-f(x_0)}{t}$\n",
    "\n",
    "In $\\mathbb{R}^2$ for instance, we fix a direction $V = (v_1,v_2)$ \n",
    "\n",
    "> $\\frac{\\delta f}{\\delta v}(x_0,y_0) = \\lim_{t\\to0}\\frac{f((x_0,y_0)+t.(v_1,v_2))-f(x_0,y_0)}{t} = \\lim_{t\\to0}\\frac{f(x_0+t.v_1,y_0+t.v_2)-f(x_0,y_0)}{t}$\n",
    "\n",
    "If we choose $v = (1,0)$\n",
    "\n",
    "> $\\frac{\\delta f}{\\delta v}(x_0,y_0) = \\lim_{t\\to0}\\frac{f(x_0+t.v_1,y_0)-f(x_0,y_0)}{t} = \\frac{\\delta f}{\\delta x}(x_0,y_0)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
