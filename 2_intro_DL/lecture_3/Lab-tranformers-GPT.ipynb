{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT - Text Generation\n",
    "\n",
    "Transformers are a bit complex to use and require greater expertise and above all greater computing capacity. The objective of this lab is to give you the basics on how to use GPT for text generation.\n",
    "\n",
    "A second year course is dedicated to 'attentions' and 'transformers' to deepen these notions.\n",
    "\n",
    "Some lecture:\n",
    "* [The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)\n",
    "* [GPT-3 Explained](https://towardsdatascience.com/gpt-3-explained-19e5f2bd3288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#With-Transformers\" data-toc-modified-id=\"With-Transformers-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>With Transformers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-configuration\" data-toc-modified-id=\"Model-configuration-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Model configuration</a></span></li><li><span><a href=\"#GPT2-tokenizer\" data-toc-modified-id=\"GPT2-tokenizer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>GPT2 tokenizer</a></span></li><li><span><a href=\"#GPT2-for-text-prediction\" data-toc-modified-id=\"GPT2-for-text-prediction-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>GPT2 for text prediction</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Transformers\n",
    "\n",
    "To make sure you can successfully run the latest versions of the example scripts, you have to install the library from source and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n",
    "\n",
    "`git clone https://github.com/huggingface/transformers\n",
    "cd transformers\n",
    "pip install .`\n",
    "\n",
    "Then cd in the example folder of your choice and run\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "In order to help you, you can find a lot of transformers [here](https://huggingface.co/transformers/notebooks.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:30.595641Z",
     "start_time": "2021-01-18T12:48:25.938929Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:30.601859Z",
     "start_time": "2021-01-18T12:48:30.597239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.3.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "# Initializing a configuration\n",
    "configuration = GPT2Config()\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:31.923557Z",
     "start_time": "2021-01-18T12:48:30.603840Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = GPT2Model(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:31.928623Z",
     "start_time": "2021-01-18T12:48:31.925594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.3.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the model configuration\n",
    "configuration1 = model.config\n",
    "configuration1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:31.932128Z",
     "start_time": "2021-01-18T12:48:31.930126Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.933939Z",
     "start_time": "2021-01-18T12:48:31.933706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "#model_name = \"microsoft/DialogRPT-updown\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.938991Z",
     "start_time": "2021-01-18T12:48:32.935198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'Ġtake', 'Ġaspirin', '.', 'ĠI', 'Ġlike', 'Ġchocolate']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize a sentence\n",
    "tokens = tokenizer.tokenize(\"I take aspirin. I like chocolate\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.945962Z",
     "start_time": "2021-01-18T12:48:32.941877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1011, 49550, 13, 314, 588, 11311]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode a sentence\n",
    "tokens = tokenizer.encode(\"I take aspirin. I like chocolate\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.950323Z",
     "start_time": "2021-01-18T12:48:32.947806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 I\n",
      "314  I\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0], tokenizer.decode(tokens[0]))\n",
    "print(tokens[4], tokenizer.decode(tokens[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.955095Z",
     "start_time": "2021-01-18T12:48:32.951943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 995]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello world\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.959520Z",
     "start_time": "2021-01-18T12:48:32.956525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18435, 995]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\" Hello world\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:32.963337Z",
     "start_time": "2021-01-18T12:48:32.960726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello world'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = tokenizer.decode(tokens)\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 for text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:41.511889Z",
     "start_time": "2021-01-18T12:48:36.670772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start text: Who was Emmanuel Macron ? Emmanuel Macron was a'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'response: Who was Emmanuel Macron? Emmanuel Macron was a French'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "text = \"Who was Emmanuel Macron ? Emmanuel Macron was a\"\n",
    "display(\"start text: \"+text)\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "#tokens_tensor = tokens_tensor.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# get the predicted next sub-word (in our case, the word 'man')\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "display(\"response: \"+predicted_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T12:48:43.264651Z",
     "start_time": "2021-01-18T12:48:41.513825Z"
    }
   },
   "source": [
    "# Using the past\n",
    "generated = tokenizer.encode(\"Michel RIVEILL is\")\n",
    "context = torch.tensor([generated])\n",
    "past = None\n",
    "\n",
    "for i in range(40):\n",
    "    print(i, end=\": \")\n",
    "    output, past = model(context, past_key_values=past)\n",
    "    token = torch.argmax(output[..., -1, :])\n",
    "\n",
    "    generated += [token.tolist()]\n",
    "    context = token.unsqueeze(0)\n",
    "\n",
    "    sequence = tokenizer.decode(generated)\n",
    "    print(sequence+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michel RIVEILL is an\n",
      "Michel RIVEILL is an analyst\n",
      "Michel RIVEILL is an analyst at\n",
      "Michel RIVEILL is an analyst at the\n",
      "Michel RIVEILL is an analyst at the New\n",
      "Michel RIVEILL is an analyst at the New America\n",
      "Michel RIVEILL is an analyst at the New America Foundation\n",
      "Michel RIVEILL is an analyst at the New America Foundation.\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The \"\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The \"S\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The \"Sandy\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The \"Sandy Hook\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The \"Sandy Hook Story\n",
      "Michel RIVEILL is an analyst at the New America Foundation. He is the author of The \"Sandy Hook Story.\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelWithLMHead, AutoTokenizer, tf_top_k_top_p_filtering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFAutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Michel RIVEILL is\"\n",
    "\n",
    "for _ in range(20):\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "\n",
    "    # get logits of last hidden state\n",
    "    next_token_logits = model(input_ids)[0][:, -1, :]\n",
    "\n",
    "    # filter\n",
    "    filtered_next_token_logits = tf_top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "    # sample\n",
    "    next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)\n",
    "\n",
    "    generated = tf.concat([input_ids, next_token], axis=1)\n",
    "\n",
    "    resulting_string = tokenizer.decode(generated.numpy().tolist()[0])\n",
    "    \n",
    "    print(resulting_string)\n",
    "    \n",
    "    sequence=resulting_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
