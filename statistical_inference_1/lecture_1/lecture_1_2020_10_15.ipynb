{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COURSE 1 - Statistical Inference 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*October, 15th, 2020*\n",
    "\n",
    "**Teacher**: Christine Malot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Practice Courses will be covered in Statistical Inference 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Structure of Class\n",
    "\n",
    "1. <u>Estimation</u>\n",
    "    - Introduction\n",
    "    - Point estimation\n",
    "    - Confidence interval\n",
    "<hr>\n",
    "2. <u>Linear model (simple)</u>\n",
    "    - **Context**: one explanatory variable $X$, one response variable $Y$ (both variables are numeric)\n",
    "    - We want to study if there exists a linear connection between $X$ and $Y$ given:\n",
    "> $Y=a+b.X+\\epsilon$ If not linear the goal is to find the 'best' $f$ function such that $Y=f(x)+\\epsilon$\n",
    "> \n",
    "> $\\epsilon$ is *noise*.\n",
    "    - estimation of a and b and of $\\sigma^2$\n",
    "    - quality of the model\n",
    "    - introduction to testing\n",
    "<hr>\n",
    "3. <u>Tests</u>\n",
    "    - Tests for parameters\n",
    "    - Goodness-of-fit tests (Shapiro, Chi-square, Kolmogorov-Smirnov)\n",
    "    - Test of independency (Chi-square)\n",
    "<hr>\n",
    "4. <u>Multiple linear model</u>\n",
    "    - **Context**: one response variable $Y$, $p$ explanatory variable $X^1, ..., X^n$\n",
    "    - **Scope**: to see if the connection that may exist between\n",
    "    - **Model**: $Y=a_0+a_1.X^1+...+a_p.X^p+\\epsilon$\n",
    "> 1. Estimation\n",
    "> 2. Quality of the model\n",
    "> 3. Testing \n",
    "> 4. Variable selection: sparsity score\n",
    ">    - Step by step selection\n",
    ">    - Penalized selection (Ridge regression, Lasso regression)\n",
    "<hr>\n",
    "5. <u>Analysis of Variance (ANOVA)</u>\n",
    "    - Generalization of multiple linear Regression when the explanatory variables are qualitative\n",
    "    - Application of Chapter 4 + multiple esting\n",
    "<hr>\n",
    "6. <u>Introduction to CART</u> (Classification and Regression Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1, Part 1- Estimation\n",
    "\n",
    "In statistics, we have observations and thanks to those observations, we wish to infer the distribution of their \"associated\" random variable (<font color=\"red\">/!\\ singular</font>).\n",
    "\n",
    "> observations: $x_1, x_2, ..., x_n$\n",
    ">\n",
    "> We assume that $x_i$ is an observation of a random variable $X_i$ where $X_1,...,X_n$ are ***independent and identically distributed (i.i.d.) random variables***.\n",
    "\n",
    "The **goal** of estimation is to develop an idea about the actual distribution of $X_1$.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Estimation of the density (continuous case). \n",
    "i.e. <font color=\"blue\">**We can identify the family of distribution of $X_1$ via histograms**</font>\n",
    "\n",
    "### 1.1. Steps to construct an histogram:\n",
    "    \n",
    "1. We construct some classes $[l_0,l_1[$, $[l_1,l_2[$, ..., $[l_{k-1},l_k[$ with $l_0 < min(x_1,...,x_n)$ and $l_k > max(x_1,...,x_n)$\n",
    "2. K is an integer such that $K \\approx 1+3.22*log_{10}(n)$ (*Sturge*'s Rule)\n",
    "3. We have to compute the '*empirical density*'\n",
    "\n",
    "|Classes|Counts|Empirical densities|\n",
    "|:----------|---:|---:|\n",
    "|$[l_0,l_1[$|$n_1$|$d_1$|\n",
    "|...|...|...|\n",
    "|$[l_{k-1},l_k[$|$n_k$|$d_k$|\n",
    "\n",
    "- With:\n",
    "    - $d_i=\\frac{n_i}{n}.\\frac{1}{l_i-l_{i-1}}$ \n",
    "    - $\\frac{1}{l_i-l_{i-1}}$ the normalized factor to take into account the length of the class\n",
    "    - $n_i$ the number of observations inside $[l_{i-1},l_i[$.\n",
    "\n",
    "<u>Notes:</u> In French, counts/frequencies is translated as 'effectifs'.\n",
    "\n",
    "<u>Remark:</u> The area of the histogram is $\\sum_{k=1}^K (l_k - l_{k-1}).d_k=1$: the area associated to a density function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example in R:</u>\n",
    "\n",
    "```R\n",
    "library(stats) \n",
    "#https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html\n",
    "A <- rexp(5000,4)\n",
    "hist(A) # wrong\n",
    "x=seq(0,10,0.01)\n",
    "plot(x,dexp(x,4),type='l')\n",
    "hist(A,freq=F)\n",
    "#q quantile\n",
    "#d density\n",
    "#r random observations```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Sliding windows (generalization of histograms):</u>\n",
    "\n",
    "> h -> bandwidth\n",
    "\n",
    "Let's consider a point $x$, the sliding window is:\n",
    "\n",
    "> $[x-\\frac{h}{2},x+\\frac{h}{2}]$\n",
    "\n",
    "The sliding window is one of several **kernel methods** (density function in the R software) such that:\n",
    "\n",
    "> $\\hat{f}(x) = C*\\sum_{i=1}^nK(\\frac{x-x_i}{h})$ \n",
    "> \n",
    "> with $K$ the kernel function (generalization of the sliding window)\n",
    "\n",
    "<u>Remark:</u> A Gaussian is a type of kernel method. \n",
    "> Gaussian Kernel: $K(y)=\\frac{1}{\\sqrt{2.\\pi}}.e^{\\frac{-y^2}{2}}$\n",
    "\n",
    "<u>Conclusion:</u>\n",
    "\n",
    "With an histogram, we identify a **family of distribution** for $X_1$. Afterwards, we have to consider a parametric family and we want to estimate the parameters of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Finding the parametric family of the distributions $X_1$\n",
    "\n",
    "Let's define a parametric family of distribution $\\{P_\\theta, \\theta \\in \\Theta\\}$\n",
    "\n",
    "<u>Examples:</u>\n",
    "> **Exponential** family: $\\{\\mathbb{E}(\\lambda), \\lambda \\in \\mathbb{R}\\ast_+\\}$\n",
    ">\n",
    "> **Gaussian** family: $\\{N_{\\mu,\\sigma^2}=N(\\mu,\\sigma^2) | \\mu\\in\\mathbb{R},\\sigma > 0\\}$ with $\\theta =(\\mu,\\mathbb{R})$ and $\\Theta=\\mathbb{R}x\\mathbb{R}\\ast^+$\n",
    ">\n",
    "> **Bernouilli** family: $\\{B_p = B(p) | p \\in [0,1]\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Estimation of $\\theta$: Scope\n",
    "\n",
    "<u>Definition of **Estimator**:</u> \n",
    "\n",
    "1. Let $X_1,...,X_n$ i.i.d. r.v. with $X_1 \\sim P_\\theta$ with $P_\\theta \\in \\{P_\\mu,\\mu\\in \\Theta\\}$\n",
    "\n",
    "2. An **estimator of $\\theta$**, denoted $\\hat{\\theta}_n$, is a **random variable defined as a function of $X_1,...,X_n$**\n",
    "\n",
    "3. If we have observations of $X_1,...,X_n$, we should be able to computed the associated value of $\\hat{\\theta}_n$\n",
    "\n",
    "4. The associated value of $\\hat{\\theta}_n$ is called an **estimation of $\\theta_n$**\n",
    "\n",
    "<u>Remark:</u> An estimation of $\\theta$ is an observation of an estimator of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Estimators of $\\theta$: Unbiased Estimators\n",
    "$\\Rightarrow$ We want an <font color=\"blue\">**unbiased estimators**</font> for estimator.\n",
    "\n",
    "<u>Definition of **Unbiased Estimator**</u>: \n",
    "\n",
    "1. Let $\\hat{\\theta}_n$ an estimator of $\\theta$\n",
    "\n",
    "2. Let $b_n(\\hat{\\theta}_n)=\\mathbb{E}[\\hat{\\theta}_n]-\\theta$ with $\\theta$ the **non-random bias** of $\\hat{\\theta}_n$\n",
    "\n",
    "3. We say that $\\hat{\\theta}_n$ is an **unbiased estimator** if:\n",
    "\n",
    "> $\\forall n \\in \\mathbb{N}$, $b_n(\\hat{\\theta}_n)=0 \\Rightarrow \\forall n \\in \\mathbb{N}$, $\\mathbb{E}[\\hat{\\theta}_n]=\\theta$\n",
    "\n",
    "4. We say that $\\hat{\\theta}_n$ is **asymptotically an unbiased estimator** of $\\theta$ is $lim_{n\\rightarrow+\\infty}b_n(\\hat{\\theta}_n)=0 \\Leftrightarrow lim_{n\\rightarrow+\\infty}\\mathbb{E}[\\hat{\\theta}_n]=\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Estimators of $\\theta$: Computing Estimators\n",
    "\n",
    "1. <u>**Is $\\bar{\\mu_n}$ an unbiased estimator of $\\mu$?**</u>\n",
    "\n",
    "With the **Law of Large Numbers**, $X_1,...,X_n$ i.i.d. r.v. with $\\mathbb{E}[X_1]=\\mu$ and $\\mathbb{V}[X_1]=\\sigma^2$\n",
    "\n",
    "> $\\bar{X} = \\frac{X_1+...+X_n}{n} \\rightarrow \\mu$ \n",
    "\n",
    "By this result, we can think $\\bar{\\mu_n} = \\bar{X_n}$ as an estimator of $\\mu$.\n",
    "\n",
    "> **$\\bar{X_n}$ is a function of $X_1,...,X_n$ and if we have observations $X_1,...,X_n$ for $X_1,...,X_n$ then we can compute the value of $\\bar{\\mu_n}$.**\n",
    "> \n",
    "> and $\\bar{X_n}$ is an estimator\n",
    "\n",
    "$\\mathbb{E}[\\bar{X}_n] = \\mathbb{E}[\\frac{X_1+...+X_n}{n}] = \\mathbb{E}[\\frac{1}{n}\\sum_{i=1}^{N}X_i] = \\frac{1}{n}.\\mathbb{E}[\\sum_{i=1}^{N}X_i]\\\\ \n",
    "\\mathbb{E}[\\bar{X}_n] = \\frac{1}{n}.\\mathbb{E}[\\sum_{i=1}^{N}X_1] = \\mathbb{E}[X_1] = \\mu$\n",
    "\n",
    "> $\\Rightarrow \\bar{X_n}$ is an unbiased estimator for $\\mu$.\n",
    "\n",
    "<u>Proposition</u>: Let $X_1,...,X_n$ i.i.d. r.v., $\\bar{X_n}$ is always an unbiased estimator of $\\mathbb{E}[X_1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <u>**Is $s_n^2$ an unbiased estimator of $\\sigma^2$?**</u>\n",
    "\n",
    "With the **Law of Large Numbers**, we ask:\n",
    "\n",
    "> $S^2_n = \\frac{1}{n}.\\sum_{i=1}^{N}(X_i-\\bar{X_n}) \\underset{?}{\\rightarrow} \\sigma^2$\n",
    "\n",
    "<u>Remark:</u> \n",
    "\n",
    "$S^2_n = \\frac{1}{n}.\\sum_{i=1}^{N}(X_i-\\bar{X_n}) = \\frac{1}{n}.\\sum_{i=1}^{N}(X_i^2-2.X_i\\bar{X_n}+\\bar{X_n}^2) \\\\\n",
    "S^2_n = \\frac{1}{n}.\\sum_{i=1}^{N}(X_i^2)-2.\\bar{X_n}.\\frac{1}{n}.(\\sum_{i=1}^{N}X_i)+n.\\frac{1}{n}\\bar{X_n}^2 \\\\\n",
    "S^2_n = \\frac{1}{n}.\\sum_{i=1}^{N}(X_i^2)-2.\\bar{X_n}.\\bar{X_n}+n.\\frac{1}{n}\\bar{X_n}^2 \\\\\n",
    "S^2_n = \\frac{1}{n}.(\\sum_{i=1}^{N}X_i^2)-\\bar{X_n}^2 \\\\\n",
    "S^2_n = \\mathbb{E}[X_1^2]-(\\mathbb{E}[X_1])^2 \\\\\n",
    "S^2_n = \\frac{1}{n}.(\\sum_{i=1}^{N}\\mathbb{E}[X_i^2]-n.\\mathbb{E}[\\bar{X_n}^2]) \\\\\n",
    "S^2_n = \\frac{1}{n}.(\\mathbb{V}[X_i]+(\\mathbb{E}[X_i])^2 - n.(\\mathbb{V}[\\bar{X_n}]+(\\mathbb{E}[\\bar{X_n}])^2)\\\\\n",
    "S^2_n = \\frac{1}{n}.(n.(\\sigma^2+\\mu^2) - n.(\\frac{\\sigma^2}{n}+\\mu^2)) \\\\\n",
    "S^2_n = \\frac{1}{n}.(n.\\sigma^2+n.\\mu^2 - n.\\frac{\\sigma^2}{n}+n.\\mu^2)\\\\\n",
    "S^2_n=\\frac{n-1}{n}\\sigma^2$\n",
    "\n",
    "We can think of $s_n^2$ as a **biased** estimator for $\\sigma^2$.\n",
    "\n",
    "$\\mathbb{E}[S^2_n]=\\mathbb{E}[\\frac{1}{n}.\\sum_{i=1}^n(X_i-\\bar{X_n})^2]=\\frac{1}{n}.\\mathbb{E}[\\sum_{i=1}^n(X_i-\\bar{X_n})^2]\\\\\n",
    "\\mathbb{E}[S^2_n]=\\frac{1}{n}.\\mathbb{E}[\\sum_{i=1}^nX_i^2-n\\bar{X_n}^2]$\n",
    "\n",
    "<u>Remark:</u> $\\mathbb{E}[S^2_n]\\neq\\sigma^2 \\rightarrow$ \n",
    "\n",
    "> $S^2_n$ is not an unbiased estimator of $\\sigma^2$ \n",
    "> \n",
    "> but $\\mathbb{E}[S^2_n]$ is asymptotically an unbiased estimator of $\\sigma^2$\n",
    "\n",
    "$\\mathbb{E}[S^2_n] \\underset{n\\rightarrow+\\infty}{\\rightarrow}\\sigma^2$\n",
    "\n",
    "<u>How to get an unbiased estimator for $\\sigma^2$ thanks to $S_n^2$</u>\n",
    "\n",
    "We see that: $\\mathbb{E}[S^2_n]=\\sigma^2-\\frac{\\sigma^2}{n}$\n",
    "\n",
    "So: $\\mathbb{E}[S^2_n+\\frac{\\sigma^2}{n}]=\\sigma^2$\n",
    "\n",
    "If we denote $\\tilde{S}_n^2=S_n^2+\\frac{\\sigma^2}{n}$ so $\\tilde{S}_n^2$ is an unbiased estimator for $\\sigma^2$?\n",
    "\n",
    "*Can we deduce an unbiased estimator of $\\sigma^2$ from the last equation?*\n",
    "\n",
    "***NO!***\n",
    "\n",
    "Although we have $\\mathbb{E}[\\tilde{S}_n^2]=\\sigma^2$ but $\\tilde{S}_n^2$ depends on $X_i,...,X_n$ and $\\sigma^2$, which are unknown.\n",
    "\n",
    "$\\rightarrow \\tilde{S}_n^2$ is not an estimator because it depends on unknown parameters. \n",
    "\n",
    "BUT: We have $\\mathbb{E}[S^2_n]=\\frac{n-1}{n}.\\sigma^2$\n",
    "\n",
    "$\\mathbb{E}[S^2_n]=\\frac{n-1}{n}.\\sigma^2 \\\\\n",
    "\\mathbb{E}[\\frac{n}{n-1}S^2_n]=\\sigma^2$\n",
    "\n",
    "We denote $\\hat{S_n^2}=\\frac{n}{n-1}.S_n^2=\\frac{1}{n-1}.\\sum_{i=1}^{N}(X_i-\\bar{X_n})^2$\n",
    "\n",
    "$\\Rightarrow \\hat{S_n^2}$ is an unbiased estimator of $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other. Notes based on Matthew Brenneman's lecture on Youtube [here](https://www.youtube.com/watch?v=ueuSH7-ufB0)\n",
    "    \n",
    "<u>Definition of **inference**:</u>\n",
    "\n",
    "Generalization based on empirical observations. In statistics to draw conclusions about **populations** based on **samples**.\n",
    "\n",
    "*types of statistical inferences*: determining the type of distribution of an rv., estimating the parameters of a known distribution, hypothesis testing about the value of a parameter for a RV, model building (create statistical model for relationship between multiple rv.).\n",
    "\n",
    "<u>Definition of **test statistic**:</u>\n",
    "\n",
    "A statistic is anything that we compute from data). It is a *test* statistic because it will be used to test an hypothesis.\n",
    "> The test statistic for the population mean $\\mu$ is the sample mean $\\bar{X}$\n",
    "\n",
    "The test statistic **is a random variable** and **has its own distribution**.\n",
    "\n",
    "<u>Definition of **sampling distribution**:</u>\n",
    "\n",
    "The distribution of the test statistic is the sampling distribution under the assumption the hypothesis is true.\n",
    "\n",
    "<u>Definition of **hypothesis testing**:</u>\n",
    "\n",
    "Determination of \"how likely\" it would be to get test statistics form the sample given the hypothesis is true (e.g. compute percentile for test statistic).\n",
    "\n",
    "<hr>\n",
    "\n",
    "<u>Example of hypothesis testing</u>\n",
    "1. Suppose rv. $X$ comes from a certain type of distribution (e.g. normal standard)\n",
    "2. Hypothesis is $\\mu = 0$\n",
    "3. Take random sample from population $X_1,...,X_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
