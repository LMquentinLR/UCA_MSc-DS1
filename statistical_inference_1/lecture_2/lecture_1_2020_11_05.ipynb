{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COURSE 1 - Statistical Inference 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*October, 15th, 2020*\n",
    "\n",
    "**Teacher**: Christine Malot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Practice Courses will be covered in Statistical Inference 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Estimation\n",
    "\n",
    "An estimator is a function of a random variable.\n",
    "\n",
    "$X_1...X_n$ IID Random Variables. We assume all the $X_i ~ P(\\theta)$ with $\\theta$ an unknown parameter.\n",
    "\n",
    "We want to estimate $\\theta$. As such, we consider an estimator $\\hat{\\theta_n}$ with $\\hat{\\theta_n} = f(X_1, ..., X_n)$ \n",
    "\n",
    "We prefer unbiased estimators. I.e. $E[\\hat{\\theta_n}] = \\theta$. \n",
    "\n",
    "We can construct assymptotically unbiased estimators. I.e. $E[\\hat{\\theta_n}] \\underset{n\\rightarrow\\infty}{\\rightarrow} \\theta$\n",
    "\n",
    "If we have #X_1, ..., X_n$ iid such that $\\theta - E[X_1], sigma^2=V[X]$\n",
    "\n",
    "an unbiased estimator for \\theta is hat theta n = 1\\n sum X_i = bar X_n\n",
    "\n",
    "an unbiased estimator for \\sigma^2 is hat sigma ^2 n = 1/n-1 \\sum (X_i - bar x_n)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second property for estimators is the convergence or consistency. Let hat theta n an estimator of theta, we say that hat theta n is consistent (or convergence) if\n",
    "\n",
    "$\\hat{\\theta_n} \\underset{P or AS}{\\rightarrow} \\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{\\theta_n}$ be an estimator of \\theta if \n",
    "\n",
    "- \\theta unbiased\n",
    "- v[$\\hat{\\theta_n}$]\\rightarrow n\\rightarrow\\infty 0 \n",
    "\n",
    "then $\\hat{\\theta_n}$ is consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let $X_1...X_n$ IID Random Variables. We denote by $\\theta$ their expectation. We know that $\\hat{\\theta_n} = \\bar{X_n}$ is an unbiased estimator for $\\theta$. Is $\\hat{\\theta_n}$ consistent? **Yes**\n",
    "\n",
    "V[\\bar{X_n}] = V[\\frac{1}{n}\\sumX_i] = 1\\n^2 V[sum X_i] = 1/n^2 * n * V[X_i] = 1/n * V[X_n]\n",
    "\n",
    "V[\\bar{X_n}] \\rightarrow n\\rightarrow\\infty 0\n",
    "\n",
    "Thanks to the previous theorem, we conclude that \\bar{X_n} is consistent.\n",
    "\n",
    "**Remark**: We can also prove that \n",
    "\n",
    "- \\hat{\\sigma^2} \\underset{P}{\\rightarrow} \\sigma^2\n",
    "\n",
    "\\hat{\\sigma^2} \\underset{AS}{\\rightarrow} \\sigma^2\n",
    "\n",
    "**Remark 2**: We can get more properties in particular, we hae some result about the best unbiased estimator.\n",
    "\n",
    "If we just consider the family of unbiased estimator of an unknown [GO BACK] If we just consider the family of unbiased estimator of an unknown parameter \\theta, there exists a 'best estimator' which is the one with the smallest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest variance is known, this is the Cramer Rao bound. This bound is connected with the Fisher Information.\n",
    "\n",
    "Cramer-Rao bound = 1/Fisher Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main techniques:\n",
    "    \n",
    "- method of moments\n",
    "- maximum likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method of moments\n",
    "\n",
    "**Remember**: X is a random variable. Thus \n",
    "- $E[X^k]$: the moment of order k\n",
    "- $E[(X - E[X])^k]$ the centered moment of order k\n",
    "\n",
    "Thanks to this and the law of large numbers, let consider $X_1, ..., X_n$, a n sample (n random variables that are IID) with distribution $P_{\\theta}$. We want to estimate $\\theta$.\n",
    "\n",
    "**Method**:\n",
    "\n",
    "1. We find $k \\in \\mathbb{N}\\ast$ such that $E[X_1^k] = g(\\theta)$\n",
    "2. An estimator of $\\hat{\\theta_n}$ is such that $g(\\hat{\\theta_n})=\\frac{1}{n}\\sum X_i^k$\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "We consider $y_1=X_1^k, ..., y_n=X_n^k$ with $y_1, ..., y_n$ IID rv. such that $E[y_1] = g(\\theta)$ with $g(\\theta)=\\mu$ unknown.\n",
    "\n",
    "$\\Rightarrow$ an estimator of $\\mu$ is just $\\frac{1}{n}\\sum X_i^k$\n",
    "\n",
    "**Remark**:\n",
    "\n",
    "Because of the law of large numbers, we know that $\\frac{1}{n}\\sum X_i^k \\underset{as}{\\rightarrow} g(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**:\n",
    "\n",
    "$X_1, ..., X_n$ IID with distribution $U([0,\\theta])$. How to estimate $\\theta$?\n",
    "\n",
    "With the method of moments:\n",
    "\n",
    "$E[X_1] = \\frac{\\theta}{2} = g(\\theta)$ with $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ and $x \\rightarrow \\frac{x}{2}$ we have g(hatthetan) = 1/n \\sum X_i$\n",
    "\n",
    "It means that \\hat{\\theta_n} = \\bar{X_n} \\Rightleftarrow \\hat{\\theta_n} = 2*\\bar{X_n}$\n",
    "\n",
    "**remark**:\n",
    "\n",
    "We have E[X_1^2] = V[X_1] + (E[X_1])^2 = theta^2/12 + \\theta^2/4 = \\theta^2/3\n",
    "\n",
    "We have that an estimator $\\hat{\\theta_{n,2}}$ is a solution of h(\\hat{\\theta_{n,2}}) = 1/n sum X_i^2\n",
    "\n",
    "\\hat{\\theta_{n,2}}^2/3 = 1/n \\sum X_i thus \\theta_{n,2} = \\sqrt{3/n * \\sum X_i^2} assuming $\\theta > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark**: In the method of moments, we can use the centered moments.\n",
    "\n",
    "The method is:\n",
    "\n",
    "1. we find k such that $E[(X - E[X])^k] = g(\\theta)$\n",
    "2. an estimator $\\hat{\\theta_n}$ is such that $g(\\hat{\\theta_n}) = \\frac{1}{n}\\sum(X_i-\\bar{X_n})^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**:\n",
    "    \n",
    "X_1, ..., X_n IID rv. with distribution $U([-\\theta,\\theta])$. We have:\n",
    "\n",
    "- $E[X_1] = 0$\n",
    "- $V[X_1] = \\frac{\\theta^2}{3}$\n",
    "\n",
    "$\\Rightarrow \\hat{\\theta_n^2} = \\frac{1}{n}\\sum(X_i-\\bar{X_n})^2$\n",
    "$\\Rightarrow \\hat{\\theta_n} = \\sqrt{\\frac{3}{n}\\sum(X_i-\\bar{X_n})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: With the method of moments, we can construct several estimators for a same parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "**The idea**: In the discrete case, we observe $x_1, ..., x_n$ that are realizations of $X_1, ..., X_n$ IID rv.\n",
    "\n",
    "We want to compute $P(X_1=x_1, ..., X_n=x_n) = \\prod{P(X_i=x_i)} = f(\\theta)$\n",
    "\n",
    "When we know the value of $\\theta$, we look at the point $(x_1, ..., x_n)$ with the biggest probability.\n",
    "\n",
    "Because we observe $(x_1, ..., x_n)$ the estimator of \\theta will be the value such that the probability to observe $(x_1, ..., x_n)$ is maximal. \n",
    "\n",
    "**definition of the maximum likelihood estimator**: Let $X_1, ..., X_n$ IID rv. with distribution $P_{\\theta}$. An estimator of $\\theta$ by maximum likelihood is $\\hat{\\theta_n}$ such that $L(\\hat{\\theta_n}) = \\underset{\\theta}{max} L(\\theta)$ where\n",
    "\n",
    "$L$: The likelihood\n",
    "\n",
    "$L(\\hat{\\theta_n}) = L(X_1, ..., X_n, \\theta)$\n",
    "\n",
    "$L(\\hat{\\theta_n}) = \\prod P(X_i=x_i)$ if $X_i$ are discrete\n",
    "\n",
    "$L(\\hat{\\theta_n}) = \\prod f_{\\theta}(X_i)$ if $X_i$ are continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**:\n",
    "\n",
    "$X_1, ..., X_n$ IID rv. with distribution Bernouilli with parameter $p$. We want to estimate $p$.\n",
    "\n",
    "$L(p) = \\prod P(X_i=x_i)$\n",
    "\n",
    "$P(X_i=0)=1-p$ and $P(X_i=0)=p \\rightarrow (1-p)^{1-X_i} * p^{X_i}$\\\n",
    "\n",
    "$L(p) = \\prod (1-p)^{1-X_i} * p^{X_i}$\n",
    "\n",
    "$L(p) = \\prod \\frac{p}{1-p}^{\\sum x_i} * (1-p)^n$\n",
    "\n",
    "We want to maximize this function as a function of $p$.\n",
    "\n",
    "In general, we do not maximize the likelihood but the log-likelihood.\n",
    "\n",
    "**remark**: log likelihood = lnL\n",
    "\n",
    "We denote by g this likelihood function\n",
    "\n",
    "$g(p) = lnL(P) = \\sum x_i * ln(p/(1-p))+n*ln(1-p)$\n",
    "\n",
    "We want to maximize g and thus compute g' \n",
    "\n",
    "$ g'(p) = \\sum x_i * (1/p + 1/(1-p))$ because $ln(1-p) = h(p) = f \\rho l(p) with l(p) = 1-p and f(p) = ln(p)$\n",
    "\n",
    "$h'(p) = f'(l(p)) * l'(p)$\n",
    "\n",
    "$g'(p) = \\sum x_i * \\frac{1}{p*(1-p)} - \\frac{n}{1-p}$\n",
    "\n",
    "we need to solve $g'(p) = 0$\n",
    "\n",
    "$\\sum x_i * \\frac{1}{p*(1-p)} - \\frac{n}{1-p} = 0$\n",
    "\n",
    "$\\sum x_i/n = p$\n",
    "\n",
    "We have to compute $g''$ and see if $g''(\\sum x_i/n) < 0$\n",
    "\n",
    "We do not make the computation but it is the case\n",
    "\n",
    "we conclude that $\\hat{p_n} = \\sum X_i/n$ is the maximum likelihood estimator for p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**: \n",
    "\n",
    "$X_1, ..., X_n$ IID rv. with distribution $E(\\lambda)$\n",
    "\n",
    "$L(\\lambda) = \\prod f_{\\lambda}(X_i) = \\prod (\\lambda * e^{-\\lambda x_i} |_{]0, +\\infty[}(x_i)) = \\lambda^n * e^{-\\lambda\\sum x_i} |_{min(x_i) > 0}$ with $|_{min(x_i) > 0}$ which does not depend on $\\lambda$\n",
    "\n",
    "We just need to maximize $h(\\lambda) = \\lambda^n*e^{-\\lambda\\sum x_i}$\n",
    "\n",
    "$l(\\lambda) = ln(h(x)) = n * ln(\\lambda) - \\lambda \\sum x_i = \\frac{n}{\\lambda} - \\sum x_i$ then $l'(\\lambda) = 0 \\Leftrightarrow \\lambda = \\frac{n}{\\sum x_i} = \\frac{1}{\\bar{x_n}}$\n",
    "\n",
    "Since we can see that $\\frac{1}{\\bar{x_n}}$ is associated to the maximization of the likelihood, we conclude the maximum likelihood estimator for $\\lambda$ is $\\hat{\\lambda} = \\frac{1}{\\bar{X_n}}$\n",
    "\n",
    "**Remark**: If we apply the method of moments with the moment of order 1, we obtain the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**:\n",
    "\n",
    "$X_1, ..., X_n$ IID rv with $U([0,\\theta])$ estimation of $\\theta$? \n",
    "\n",
    "$L(\\theta) = \\prod \\frac{1}{\\theta} |_{[0,\\theta]} (x_i) = \\frac{1}{\\theta^n} |_{min(x_i)\\ge0} * |_{max(x_i)\\le\\theta}$ We cannot make the maximization by making a derivation because $|_{max(x_i)\\le\\theta}$ depends on $\\theta$ but we can look into the behaviour.\n",
    "\n",
    "We conclude that in this case the maximum estimator of $\\theta$ is $\\hat{\\theta_n} = max(X_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "\n",
    "Let $X_1, ..., X_n$ IID rv. with $U([0,\\theta])$, we see that $\\hat{\\theta_n} = max(X_i)$ is an estimator of $\\theta$.\n",
    "\n",
    "1. Is $\\hat{\\theta_n}$ an unbiased estimator?\n",
    "2. If not, deduce an unbiased estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an unknown parameter $\\theta$, we can construct several estimators.\n",
    "\n",
    "How to know which is the best?\n",
    "\n",
    "**definition**: quadratic error.\n",
    "\n",
    "Let $\\hat{\\theta_n}$ an estimator of $\\theta$. The quadratic error of $\\hat{\\theta_n}$ is:\n",
    "\n",
    "$QE(\\hat{\\theta_n}) = E[(\\hat{\\theta_n}-\\theta)^2]$\n",
    "\n",
    "**property:**\n",
    "\n",
    "$QE[\\hat{\\theta_n}] = V[\\hat{\\theta_n}] + (b(\\hat{\\theta_n}))^2$ with $b(\\hat{\\theta_n}) = E[\\hat{\\theta_n}] - \\theta$\n",
    "\n",
    "**proof**:\n",
    "\n",
    "INSERT PICTURE\n",
    "\n",
    "$QE[\\hat{\\theta_n}] = V[\\hat{\\theta_n}] + 2b(\\hat{\\theta_n}) * E[(\\hat{\\theta_n}-E[\\hat{\\theta_n}])] + (b(\\hat{\\theta_n}))^2 =  V[\\hat{\\theta_n}] + 2b(\\hat{\\theta_n}) * E[(\\hat{\\theta_n}]-E[E[\\hat{\\theta_n}]] + (b(\\hat{\\theta_n}))^2 = V[\\hat{\\theta_n}] + 2b(\\hat{\\theta_n}) * E[(\\hat{\\theta_n}]-E[\\hat{\\theta_n}] + (b(\\hat{\\theta_n}))^2 = V[\\hat{\\theta_n}] + (b(\\hat{\\theta_n}))^2$\n",
    "\n",
    "**Remark**: \n",
    "\n",
    "If $\\bar{\\theta_n}$ is an unbiased estimator of $\\theta$ then $b4(\\bar{\\theta_n})$ and $QE(\\bar{\\theta_n}) = V[\\bar{\\theta_n}]$.\n",
    "\n",
    "Because of the definition of the quadratic error, we prefer unbiased estimator with a small variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between 2 estimators\n",
    "\n",
    "Let $\\bar{\\theta_{n,1}}$ and $\\bar{\\theta_{n,2}}$ two estimators of $\\theta$. if $\\forall n, QE(\\bar{\\theta_{n,1}}) \\le QE(\\bar{\\theta_{n,2}})$\n",
    "\n",
    "**Remark**: If we have two unbiased estimators for $\\theta$, a way to get a better estimator consist in considering aggregated estimators. It means: $\\bar{\\theta_{n,1}}$ and $\\bar{\\theta_{n,2}}$ 2 estimators of \\theta. $\\bar{\\theta_{n,3}} = a.\\bar{\\theta_{n,1}} + b.\\bar{\\theta_{n,2}}$.\n",
    "\n",
    "We find ~a and ~b such that QE(~a\\bar{\\theta_{n,1}} + ~b\\bar{\\theta_{n,2}}) = min a,b QE(a\\bar{\\theta_{n,1}} + b\\bar{\\theta_{n,2}})\n",
    "\n",
    "The best linear estimator constructed with $\\bar{\\theta_{n,1}}$ and $\\bar{\\theta_{n,2}}$ is ~a\\bar{\\theta_{n,1}} + ~b\\bar{\\theta_{n,2}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "Let $X_1, ..., X_n$ IID rv. with $U([0,\\theta])$\n",
    "\n",
    "1. $\\hat{\\theta_{n,1}} = 2*\\bar{X_n}$ Compute the quadratic error of $\\hat{\\theta_{n,1}}$\n",
    "2. $\\hat{\\theta_{n,2}}$ ther unbiased version of $Max(X_i)$ Compute the quadratic error of $\\hat{\\theta_{n,2}}$\n",
    "3. Which one is the better one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawback of Point Estimation\n",
    "\n",
    "Point estimation has a main drawback. It is the **fluctuation** due to the sample.\n",
    "\n",
    "Illustration of the phenomenon:\n",
    "\n",
    "Let $X_1, ..., X_n$ IID rv. with distribution $N(\\mu, \\sigma^2)$. We want to estimate $\\mu$ i.e. $\\mu = \\bar{X_n}$.\n",
    "\n",
    "We perform simulations. We create 50 datasets associated to $X_1, ..., X_n$ and on each of them we compute a value of $\\hat{\\mu_n}$.\n",
    "\n",
    "To have a quantification of the quality of the estimation, we prefer a **confidence interval**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval\n",
    "\n",
    "A confidence Interval is a **random object**. To a confidence interval is associated a **confidence level**. \n",
    "\n",
    "This confidence level is generally denoted ``100 * (1-alpha)%``\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "Let $X_1, ..., X_n$ IID rv. with distribution $P_\\theta$\n",
    "\n",
    "Let $\\hat{\\theta_n}$ an estimator of $\\theta$\n",
    "A confidence interval, based on $\\hat{\\theta_n}$, with confidence level $100 * (1-\\alpha)\\%$ is a random interval $[A_n, B_n]$ such that:\n",
    "\n",
    "- $A_n = f_1(\\hat{\\theta_n})$ and $B_n = f_2(\\hat{\\theta_n})$\n",
    "- $P(\\theta \\in [A_n, B_n]) = 1-\\alpha$\n",
    "\n",
    "**Remark**: more and more alpha is small more and more the confidence level is close to 1, and the length of the confidence interval is big.\n",
    "\n",
    "**How to construct such confidence interval**?\n",
    "\n",
    "1. $X_1, ..., X_n$ are IID rv. with a distribution $N(\\mu, \\sigma^2)$ and we assume $\\mu$ is unknown and $\\sigma^2$ is known.\n",
    "2. We need at first a point estimator for $\\mu$ i.e. $\\mu = \\bar{X_n}$ \n",
    "\n",
    "$$\\bar{X_n} \\~ N(\\mu, \\sigma^2)$$\n",
    "\n",
    "3. We want to find $A_n$ and $B_n$ random variables. that are functions of $\\bar{X_n}$ such that $P(A_n \\le \\mu \\le B_n) = 1-\\alpha$\n",
    "\n",
    "We make the choice $A_n = \\bar{X_n} - S_n$ and $B_n = \\bar{X_n} + T_n$\n",
    "\n",
    "We need to find $S_n$ and $T_n$ such that:\n",
    "\n",
    "$P(\\bar{X_n} - S_n \\le \\mu \\le \\bar{X_n} + T_n) = 1-\\alpha \\Leftrightarrow P(- T_n \\le \\bar{X_n} - \\mu \\le S_n) = 1-\\alpha$\n",
    "\n",
    "We know $ \\bar{X_n} - \\mu \\~ N(0, \\frac{\\sigma^2}{n}) \\Leftrightarrow P(\\frac{-\\sqrt{n}*T_n}{\\sigma} \\le \\sqrt{n} * \\frac{\\bar{X_n}-\\mu}{\\sigma} \\le \\frac{\\sqrt(n)*S_n}{\\sigma}) = 1-\\alpha$\n",
    "\n",
    "Several solutions exist because 2 quantities to determine with just one equation. We have to make a choice.\n",
    "\n",
    "$y_n = \\sqrt{n} * \\frac{\\bar{X_n}-\\mu}{\\sigma}$\n",
    "\n",
    "$P(y_n \\lt \\frac{-\\sqrt(n)*T_n}{\\sigma}) + P(y_n \\gt \\frac{\\sqrt{n}*S_n}{\\sigma}) = \\alpha_1 + \\alpha_2 = \\alpha$\n",
    "\n",
    "3 main choices:\n",
    "\n",
    "1.\n",
    "\n",
    "$\\alpha_1 = \\alpha_2 = \\alpha/2$ and we have $P(y_n \\lt \\frac{-\\sqrt(n)*T_n}{\\sigma}) = P(y_n \\gt \\frac{\\sqrt(n)*S_n}{\\sigma}) =\\alpha/2$ One solution is to take $\\frac{\\sqrt{n}*S_n}{\\sigma} = quantile of order 1-\\alpha/2 for N(0,1)$\n",
    "\n",
    "We denote this quantile by $Z_{1-\\alpha/2}$ and $S_n = \\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha/2}$\n",
    "\n",
    "Confidence interval for \\mu at level $100*(1-\\alpha)\\%$ is $[\\bar{X_n}-\\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha/2}, \\bar{X_n}+\\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha/2}]$\n",
    "\n",
    "2.\n",
    "\n",
    "$\\alpha_1 = 0$ and $\\alpha_1 = \\alpha$\n",
    "\n",
    "$P(y_n \\lt \\frac{-\\sqrt(n)*T_n}{\\sigma}) = 0$ and $P(y_n \\gt \\frac{\\sqrt{n}*S_n}{\\sigma}) = \\alpha$\n",
    "\n",
    "This implies $T_n = -\\infty$ and $P(y_n \\gt \\frac{\\sqrt{n}*S_n}{\\sigma}) \\Leftrightarrow P(y_n \\le \\frac{\\sqrt{n}*S_n}{\\sigma}) = 1-\\alpha$ which implies $\\frac{\\sqrt{n}*S_n}{\\sigma} = Z_{1-\\alpha}$\n",
    "\n",
    "In this case, the confidence interval becomes $[\\bar{X_n} - \\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha}, +\\infty]$\n",
    "\n",
    "3.\n",
    "\n",
    "$\\alpha_1 = \\alpha$ and $\\alpha_1 = 0$\n",
    "\n",
    "In this case, the confidence interval becomes $[-\\infty, \\bar{X_n} + \\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha}]$\n",
    "\n",
    "More generaly, you take $\\alpha_1 \\ge 0$, $\\alpha_2 \\ge 0$ such that $\\alpha_1 + \\alpha_2 = \\alpha$ and:\n",
    "\n",
    "INSERT PICTURE2\n",
    "\n",
    "the associated confidence interval is $[\\bar{X_n} - \\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha_2}, \\bar{X_n} - \\frac{\\sigma}{\\sqrt{n}}*Z_{1-\\alpha_1}] $. The length of the confidence interval is $\\frac{\\sigma}{\\sqrt{n}}*(Z_{1-\\alpha_2}+Z_{1-\\alpha_1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
