{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COURSE 1 - Statistical Inference 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*October, 16th, 2020*\n",
    "\n",
    "**Teacher**: Marco Corneli\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Remark**: Most Theoretical Courses will be covered in Statistical Inference 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Structure of Class\n",
    "\n",
    "<u>Recommended reading:</u> Larry Wasserman's <u>*All of Statistics, a Concise Course in Statistical Inference*</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "Covers Chapter 6 of <u>*All of Statistics, a Concise Course in Statistical Inference*</u>\n",
    "<hr>\n",
    "\n",
    "## 1.1.Assumptions for the class\n",
    "1. **a**) We work on a set of **independent and indentically distributed** (ie. **i.i.d.**) random variables (r.v.) $X_1, ..., X_n$. \n",
    "1. **b**) An experiment is repeated $N$ times.\n",
    "<p>\n",
    "2. $\\exists$ a **parametric model** for $X_1, ..., X_n$ such that:  \n",
    "> We denote $X_1 \\sim f(x,\\mu,\\sigma^2)$: a **probability density function** (i.e. **PDF**)\n",
    "> \n",
    "> and\n",
    "> \n",
    "> $f \\in ℱ = \\{f(x,\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2.\\pi\\sigma}}.e^{-\\frac{1}{2}.(\\frac{X-\\mu}{\\sigma})^2} \\mid \\mu \\in \\mathbb{R}, \\sigma > 0\\}$\n",
    "<p>\n",
    "3. The problem is the following: estimating both $\\mu$ and $\\sigma$.\n",
    "<p>\n",
    "4. We denote $P_{\\mu,\\sigma}\\{X_1 \\le x\\} = F_{\\mu,\\sigma}(x)$, with $F$ the **cumulative distribution function** (i.e. **CDF**)\n",
    "> Often , when $X_1$ is a continuous r.v.:\n",
    "> \n",
    "> $F(x)=\\int_{-\\infty}^{x}f(x,\\theta)dx$, with $f(.,\\theta)$ the $PDF$ of $X_i$ and $\\theta$ some parameter.\n",
    "<p>\n",
    "5. For **non-parametric models**, we know that $X_1, ..., X_n$ **i.i.d.** follow a certain CDF $F$ w/ $F$ unknown, and we always assume that $F\\in ℱ = \\{all CDFs\\}$. Example:\n",
    "> $X_1, ..., X_n$ represent coin flips, $X_1 \\sim Ber(p)$\n",
    ">\n",
    "> if $X_1 \\sim Ber(p) \\Rightarrow P_p(X=1)=p$, $P_p(X=0)=1-p$ and $\\mu=p$, $\\sigma=p.(1-p)$\n",
    ">\n",
    "> *Question: \"How to estimate p\"?*\n",
    "\n",
    "<hr>\n",
    "<u>Example of a Gaussian model:</u>\n",
    "\n",
    "$P(X\\le x)=F(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2.\\pi\\sigma}}.e^{-\\frac{1}{2}.(\\frac{X-\\mu}{\\sigma})^2}dx$\n",
    "\n",
    "<hr>\n",
    "<u>Example of regression:</u>\n",
    "\n",
    "You observe $(X_1,y_1),...,(X_n,Y_n)$ with: \n",
    "> $X_1$, the **features** or **covariates** of the i.i.d. r.v.\n",
    "> \n",
    "> $Y_1$ bveing the response variable or dependent variable\n",
    ">\n",
    "> <u>Note:</u> if $Y_1$ is continuous (e.g. $Y_1 \\in \\mathbb{R}$) or discreet *not bounded* (e.g. $Y_1 \\in \\mathbb{N}$) then we are in the case of a **regression**; if $Y_1$ is ordinal then we are in the case of a **classification** and $Y_1$ can be called a **label**\n",
    "\n",
    "$\\Rightarrow$ We want to model the relation(s) between $X_1$ and $Y_1$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "7. We want to focus on the value $R(x) = \\mathbb{E}(y_i|X_i=x) \\leftarrow$ quantity to model ($\\ast$)\n",
    "\n",
    "**observation**: Given the observation ($\\ast$):\n",
    "> $y_i = y_i+R(X_i)-R(X_i) = R(X_i) + y_i-R(X_i) = R(X_i) + \\epsilon_i$ \n",
    ">\n",
    "> and:\n",
    "> \n",
    "> $\\mathbb{E}[\\epsilon_i]=\\mathbb{E}[\\mathbb{E}[\\epsilon_i|X_i]] = \\mathbb{E}[\\mathbb{E}[y_i-R(X_i)|X_i]]=\\mathbb{E}[\\mathbb{E}[y_i|X_i]-R(X_i)]=0$ \n",
    "> \n",
    "> with $\\mathbb{E}[y_i|X_i]=R(X_i)$ \n",
    "\n",
    "What about R(x)?\n",
    "- **Linear Regression**: $R(x) = \\{\\beta_0+\\beta_1.X\\mid\\beta_0,\\beta_1\\in\\mathbb{R}\\}$\n",
    "<p>\n",
    "- **Polynomial Regression**: $R(x) = \\{\\beta_0+\\beta_1.X+...+\\beta_n.X^N\\mid\\beta_0,\\beta_1,...,\\beta_N\\in\\mathbb{R}\\}$\n",
    "<p>\n",
    "- **Neural Network**: $R(x) = \\{\\eta.\\begin{pmatrix}\\alpha_0+\\alpha_1.X\\\\\\beta_0+\\beta_1.X\\end{pmatrix}$.$\\begin{pmatrix}\\gamma_0\\\\\\gamma_1\\end{pmatrix}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Point Estimation\n",
    "\n",
    "In all cases, we saw there is a parameter $\\theta$ to estimate. An estimator **$\\hat{\\theta_n}$** is a random variable (depends on the data) such that:\n",
    "\n",
    "> $\\hat{\\theta_n}:=g(X_1,...,X_n)$\n",
    "<hr>\n",
    "<u>Example of linear multivariate regression:</u>\n",
    "\n",
    "$y=\\beta.X+\\epsilon$\n",
    "\n",
    "$\\Rightarrow y_i=\\beta_0+\\beta_1.X_{i,1}+...+\\beta_n.X^{i,n}$ with $[X]_{i,j} = X_{i,j}$ and $\\hat{\\beta_{ols}}=(X^T.X)^{-1}X^T.y$ (OLS: ordinary least square)\n",
    "\n",
    "<u>Example of coin toss:</u>\n",
    "\n",
    "$X_1,...,X_n\\sim Ber(p) \\Rightarrow \\hat{p}=\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}X_i = \\bar{X}$ (sample mean)\n",
    "\n",
    "Here $g(X_1,...,X_n)=\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}X_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the estimator $\\hat{\\theta_n}$ an unbiased estimator of $\\theta$?\n",
    "\n",
    "<u>*Meaning*:</u> it means $bias(\\hat{\\theta_n})=\\mathbb{E}(\\hat{\\theta_n})-\\theta$. If $bias(\\hat{\\theta_n})=0$ it means $\\hat{\\theta_n}$ is unbiased.\n",
    "<hr>\n",
    "<u>Example of coin toss:</u> Is $\\hat{p}=\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}X_i$ unbiased?\n",
    "\n",
    "$\\mathbb{E}_p[\\hat{p}]=\\mathbb{E}_p[\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}X_i] \\\\ \\mathbb{E}_p[\\hat{p}]=\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}\\mathbb{E}_p[X_i] \\\\ \\mathbb{E}_p[\\hat{p}]=\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}p=\\frac{N.p}{N}=p$\n",
    "\n",
    "$bias(\\hat{\\theta_n})=\\mathbb{E}_p[\\hat{p}]-p=p-p=0$ \n",
    "\n",
    "It is unbiased.\n",
    "<hr>\n",
    "\n",
    "<u>**Definition**:</u> $\\hat{\\theta_n}$ is a consistent (synonymous with convergent) estimator of $\\theta$ if:\n",
    "> $\\hat{\\theta_n}\\overset{P_\\theta}{\\rightarrow}\\theta$\n",
    ">\n",
    "> $\\underset{N\\rightarrow+\\infty}{lim}\\mathbb{P}\\{\\mid\\hat{\\theta_n}-\\theta\\mid\\gt\\epsilon\\}=0$\n",
    "\n",
    "<u>**Theorem**:</u> if $\\hat{\\theta_n}$ is unbiased and $\\mathbb{V}[\\hat{\\theta_n}]\\underset{N\\rightarrow{+\\infty}}{\\rightarrow}0\\Rightarrow\\hat{\\theta_n}\\overset{P_\\theta}{\\rightarrow}\\theta$\n",
    "\n",
    "<u>**Proof**:</u>\n",
    "<hr>\n",
    "\n",
    "> <u>definition:</u> $MSE(\\hat{\\theta_n})$ (Mean Squared Error) is defined as $MSE_\\theta(\\hat{\\theta_n})=\\mathbb{E}_\\theta[\\hat{\\theta_n}-\\theta]^2$\n",
    ">\n",
    "> $MSE$ is a way of choosing between two estimators\n",
    ">\n",
    "> <u>proposition:</u> $MSE_\\theta(\\hat{\\theta_N})=bias^2(\\hat{\\theta_N})+\\mathbb{V}[\\hat{\\theta_N}]$\n",
    "\n",
    "> It can be shown that: \n",
    ">\n",
    "> $\\mathbb{E}_\\theta[\\hat{\\theta_n}-\\theta]^2 = \\mathbb{E}_\\theta[(\\hat{\\theta_n}-\\bar{\\theta_n})+(\\bar{\\theta_n}-\\theta)]^2$ with $\\bar{\\theta_N}=\\mathbb{E}_\\theta[\\hat{\\theta_N}]$\n",
    ">\n",
    "> $=\\mathbb{E}_\\theta[(\\hat{\\theta_n}-\\bar{\\theta_n})^2+(\\bar{\\theta_n}-\\theta)^2+2.(\\hat{\\theta_n}-\\bar{\\theta_n}).(\\bar{\\theta_n}-\\theta)]$\n",
    ">\n",
    "> $=\\mathbb{E}_\\theta[(\\hat{\\theta_n}-\\bar{\\theta_n})^2]+\\mathbb{E}_\\theta[(\\bar{\\theta_n}-\\theta)^2]+2.\\mathbb{E}_\\theta[(\\hat{\\theta_n}-\\bar{\\theta_n}).(\\bar{\\theta_n}-\\theta)]$\n",
    ">\n",
    "> $=\\mathbb{E}_\\theta[(\\hat{\\theta_n}-\\bar{\\theta_n})^2]+\\mathbb{E}_\\theta[(\\bar{\\theta_n}-\\theta)^2]+2.\\mathbb{E}_\\theta[(\\hat{\\theta_n}-\\bar{\\theta_n}).(\\bar{\\theta_n}-\\theta)]$\n",
    ">\n",
    "> $=\\mathbb{V}_\\theta[\\hat{\\theta_n}]+(\\bar{\\theta_n}-\\theta)^2+2.(\\bar{\\theta_n}-\\theta).\\mathbb{E}[\\hat{\\theta_n}-\\bar{\\theta_n}]$\n",
    ">\n",
    "> $=\\mathbb{V}_\\theta[\\hat{\\theta_n}]+(\\mathbb{E}_\\theta[\\hat{\\theta_n}]-\\theta)^2+2.(\\bar{\\theta_n}-\\theta).\\mathbb{E}[\\hat{\\theta_n}-\\bar{\\theta_n}]$\n",
    ">\n",
    "> $=\\mathbb{V}_\\theta[\\hat{\\theta_n}]+bias^2(\\hat{\\theta_N})+2.(\\bar{\\theta_n}-\\theta).(\\mathbb{E}[\\hat{\\theta_n}]-\\bar{\\theta_n})$\n",
    ">\n",
    "> $=\\mathbb{V}_\\theta[\\hat{\\theta_n}]+bias^2(\\hat{\\theta_N})+2.(\\bar{\\theta_n}-\\theta).0$\n",
    ">\n",
    "> $=\\mathbb{V}_\\theta[\\hat{\\theta_n}]+bias^2(\\hat{\\theta_N})$\n",
    "\n",
    "<u>**Note** on Markhov's Inequality:</u>: $\\mathbb{P}\\{X\\ge t\\}\\le\\frac{\\mathbb{E}[X]}{t}$\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example:</u>\n",
    "\n",
    "$\\{X_1,...,X_N\\}\\underset{iid}{\\sim}Ber(p)$, $\\hat{p_N}=\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}X_i$, with:\n",
    "\n",
    "> $E_p[\\hat{p_N}]=P(bias(\\hat{p_N})=0)$\n",
    "> \n",
    "> $Var_p(\\hat{p_N})=Var_p(\\frac{1}{N}\\overset{N}{\\underset{i=1}{\\sum}}X_i)$\n",
    "\n",
    "> *Recall*: Usually $Var(X+Y)\\neq Var(X)+Var(Y)$ except when $X \\perp Y$ \n",
    "> \n",
    "> Then, because $X$ and $Y$ are independent $\\Rightarrow Var(X+Y)=Var(X)+Var(Y)$\n",
    "> \n",
    "> Thus: $Var_p(\\hat{p_N})=\\frac{1}{N^2}.\\overset{N}{\\underset{i=1}{\\sum}}Var_p(X_i)=\\frac{1}{N^2}.N.p.(1-p)\\\\ \\Rightarrow Var_p(\\hat{p_N})=\\frac{p.(1-p)}{N}$\n",
    "\n",
    "$Var_p(\\hat{p_N})$ converges in probability towards $\\theta$ when N goes towards $+\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "### EXERCISE 1,2,3 p95-96"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
