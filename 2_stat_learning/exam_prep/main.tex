\documentclass{homework}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{csquotes}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
}
\usepackage[utf8]{inputenc}
% \usepackage[english]{babel}

\usepackage[
backend=biber,
style=bwl-FU,
sorting=ynt
]{biblatex}

\addbibresource{sample.bib}
\usepackage{listings}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{float}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{bbm}
\usepackage{amsmath}

\title{Statistical Learning Theory Exam}
\author{Quentin Le Roux}

%%%%%%%%%%%%%%%%%% dimensions & Notation
% \mathbb{R}^d
% \mathcal{H}
% \underset{i=1}{\overset{n}{\sum}}
% \underset{}{arg\,min}
% \mathbbm{1}_{h(x_i)\neq y_i}  \mathbbm{1}_{h(x_i)\neq f(x_i)}
% \underset{n\rightarrow+\infty}{\overset{\mathbb{P}}{\rightarrow}}
% \forall i\in\{1,...,n\}
% \exists i\in\{1,...,n\}
%%%%%%%%%%%%%%%%%% ERM
% h:\mathcal{X}\rightarrow\mathcal{Y}
% \underset{h\in\mathcal{H}}{arg\,min}
% \mathcal{R}_{\mathcal{D},f}(h)=\mathbb{P}_{x\sim\mathcal{D}}(h(x)\neq f(x))
% \hat{\mathcal{R}}_{\mathcal{S}}(h)=\frac{1}{n}|\{i\in\{1,...,n\}\quad{s.t.}\quad h(x_i)\neq y_i\}|
% \frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbbm{1}_{h(x_i)\neq y_i}
%%%%%%%%%%%%%%%%%% BEGIN ALIGN
% \begin{align*}
% &=&()\\
% &=&()\\
% &=&()\\
% \end{align*}
%%%%%%%%%%%%%%%%%% EQUATION
% \begin{equation}
%   h(x) =
%     \begin{cases}
%       1 & \text{if }\\
%       0 & \text{otherwise}
%     \end{cases}       
% \end{equation}
% 
% 
% 
% 
% 
% 

\begin{document}

\maketitle

\exercise[1: ]

\subsection*{1.1}

% \exercise[2: ]

% \subsection*{2.1 }

\clearpage
\section*{Appendix}
\subsection*{1 - Definition of hypothesis class}
Given a hypothesis $h:\mathcal{X}\rightarrow\mathcal{Y}$, also called a prediction rule or predictor, we denote $\mathcal{H}$ a space of predictor functions such that, in the case of linear classifiers:
$$\mathcal{H}=\{h:sign(x)\rightarrow w^Tx+b,\, w\in\mathbb{R}^d,\, b\in\mathbb{R}\}$$
where $w^Tx$ denotes the scalar product between $w$ and $x$ such that $\underset{i=1}{\overset{d}{\sum}}w_ix_i$.

\textbf{Assumption (noiseless setting)}: $\exists\,\,f:\mathcal{X}\rightarrow\mathcal{Y}\quad{s.t.}\quad\forall x\in \mathcal{X},\,y=f(x)$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{2 - Definition of the risk of a classifier and Empirical Risk Minimization}
We define the risk of a classifier the probability that $h$ does not return the correct label on a (new) random sample:
$$\mathcal{R}_{\mathcal{D},f}(h)=\mathbb{P}_{x\sim\mathcal{D}}(h(x)\neq f(x))$$
It is also called the generalization error or true error. In the context of hypothesis classes, we want to find $h$ with small generalization error:
$$h\in\underset{h\in\mathcal{H}}{arg\,min}\,\mathcal{R}_{\mathcal{D},f}(h)=\underset{h\in\mathcal{H}}{arg\,min}\,\mathbb{P}_{x\sim\mathcal{D}}(h(x)\neq f(x))$$
Since we know neither $\mathcal{D}$ or $f$, we replace $\mathcal{R}_{\mathcal{D},f}$ by its empirical (risk) version, also called training error:
$$\hat{\mathcal{R}}_{\mathcal{S}}(h)=\frac{1}{n}|\{i\in\{1,...,n\}\quad{s.t.}\quad h(x_i)\neq y_i\}|=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbbm{1}_{h(x_i)\neq y_i}$$
\textbf{Proof of $\mathbb{E}[\hat{\mathcal{R}}_{\mathcal{S}}(h)]=\mathcal{R}_{\mathcal{D},f}(h)$}:
\begin{align*}
   \mathbb{E}[\hat{\mathcal{R}}_{\mathcal{S}}(h)]&=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbb{E}[\mathbbm{1}_{h(x_i)\neq y_i}] &(linearity)\\
   &=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbb{P}(h(x_i)\neq y_i)&(\mathbb{E}[\mathbbm{1}_A]=\mathbb{P}(A))\\
   &=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbb{P}(h(x_i)\neq f(x_i))&(noiseless\, assumption)\\
   \forall i \in \{1,...,n\},\,\mathbb{P}(h(x_i)\neq f(x_i))&=\mathbb{P}(h(x)\neq f(x))&(x_i\,are\,IID)\\
   \mathbb{E}[\hat{\mathcal{R}}_{\mathcal{S}}(h)]&=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathcal{R}_{\mathcal{D},f}(h)\\
   &=\mathcal{R}_{\mathcal{D},f}(h)
\end{align*}
\textbf{Proof of $\mathbb{E}[\hat{\mathcal{R}}_{\mathcal{S}}(h)]\underset{n\rightarrow+\infty}{\overset{\mathbb{P}}{\rightarrow}}\mathcal{R}_{\mathcal{D},f}(h)$}:\\
Given $\forall i\in\{1,...,n\},\,x_i\,{IID}$, we have $Z_i\,{IID}$ such that:
$$Z_i=\mathbbm{1}_{h(x_i)\neq f(x_i)}$$
As $Z_i$ are bounded almost surely by 1, we can use the law of large numbers and write:
\begin{align*}
\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbbm{1}_{h(x_i)\neq y_i}&\overset{\mathbb{P}}{\rightarrow}\mathbb{E}[\mathbbm{1}_{h(x_i)\neq f(x_i)}]\\
\hat{\mathcal{R}}_{\mathcal{S}}(h)&\overset{\mathbb{P}}{\rightarrow}\mathcal{R}_{\mathcal{D}, f}(h)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{3 - (Weak) Law of Large Numbers}
Let $Z_1, Z_2,...,Z_n$ a sequence of IID random variables.\\Assume that $\mathbb{E}[|Z_1|]<+\infty$ and set $\mu:=\mathbb{E}[Z_1]$, then:
$$\frac{Z_1+...+Z_n}{n}\overset{P}{\rightarrow}\mu$$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4 - Demonstration of Overfitting}
If the hypothesis class $\mathcal{H}$ is too large, it can brings the empirical risk to $0$, memorizing the examples while never generalizing.
\begin{equation*}
  h(x) =
    \begin{cases}
      y_i & \text{if $\exists i\in\{1,...,n\}\quad{s.t.}\quad x=x_i$}\\
      0 & \text{otherwise}
    \end{cases}       
\end{equation*}
This leads to:
$$\hat{\mathcal{R}}_{\mathcal{S}}(h)=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}\mathbbm{1}_{h(x_i)\neq y_i}=0$$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5 - Risk function and Error Decomposition}
We want to minimize $R_D(h)$ for $h\in \mathcal{H}$. Unfortunately, we only have access to a proxy $\hat{R}_S(h)$. The goal is to find $h_S \in \mathcal{H}$ such that $\hat{R}_S(h)$ is minimal.
\begin{align*}
R_D(h)&=\mathbb{E}_{(x,y)\sim\mathbb{D}}[l(y,h(x))]&(Generalization\,Error)\\
\hat{R}_S(h)&=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}l(y_i,h(x_i))&(Empirical\,Risk)
\end{align*}
We can decompose the error of $h_S$ in two parts: $$R_D(h_S)=\epsilon_{approx} + \epsilon_{est}$$
Where $\epsilon_{approx} = \underset{h\in\mathcal{H}}{min}\,\mathcal{R}_D(h)$ is the \textbf{approximation error} (the minimum risk achievable by a predictor in the class $\mathcal{H}$, a measure of the loss given a restriction on our model choice), and $\epsilon_{est}=\mathcal{R}_D(h_S) - \underset{h\in\mathcal{H}}{min}\,\mathcal{R}_D(h)$, the \textbf{estimation error} (the error because the empirical risk is an estimate of the true risk).

There is a \textbf{trade-off} between the two errors (approximation error is large if $\mathcal{H}$ is small, and goes to zero is it is large, and reverse with estimation error).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6 - Linear Regression}
\textbf{Least-Square Regression}:\\
Consider data point $(x_i, y_i)\in\mathbb{R}^d,\mathbb{R}$ and linear hypothesis class:
$$\mathcal{H}=\{h:x\rightarrow w^Tx+b,\quad w\in\mathbb{R}^d,\, b\in\mathbb{R}\}$$
where $w^Tx$ denotes the scalar product between $w$ and $x$ such that $\underset{i=1}{\overset{d}{\sum}}w_ix_i$.
We consider the following empirical risk minimization problem:
$$(\hat{w},\hat{b})\in\underset{w,b\,\in\,\mathbb{R}^d,\,\mathbb{R}}{arg\,min}\underset{i=1}{\overset{n}{\sum}}(y_i-w^Tx_i-b)^2$$
where we consider the ordinary least squares loss function $l(y,y') = (y-y')^2$.\\
In the context of removing the bias $b$, the empirical risk is:
$$\hat{R}_S(h)=\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}(w^Tx_i-y_i)^2=F(w)$$
where $F$ is a convex, smooth function, and we want to minimize $w\rightarrow F(w)$ with respect to $w\in\mathbb{R}^d$
\textbf{Lasso Regression}:\\
If we want a lot of coordinates to be zero, then the $l_1$ norm is appropriate. The hypothesis class is updated to:
$$\mathcal{H}_\lambda=\{h\,linear\quad s.t. \quad \lVert h\rVert_1\le\lambda\}$$
And Lasso solves:
$$(\hat{w},\hat{b})\in\underset{w,b\,\in\,\mathbb{R}^d,\,\mathbb{R}}{arg\,min}\underset{i=1}{\overset{n}{\sum}}(y_i-w^Tx_i-b)^2+\lambda\lVert w\rVert_1$$
where $\lVert.\rVert$ denotes the $L_1$ norm such that $\lVert x\rVert_1=\underset{i=1}{\overset{n}{\sum}}|x_i|$.

\textbf{Ridge Regression}:\\
If we want to keep the solutions that have a small $l_2$ norm, the hypothesis class is updated to:
$$\mathcal{H}_\lambda=\{h\,linear\quad s.t. \quad \lVert h\rVert\le\lambda\}$$
And Lasso solves:
$$(\hat{w},\hat{b})\in\underset{w,b\,\in\,\mathbb{R}^d,\,\mathbb{R}}{arg\,min}\underset{i=1}{\overset{n}{\sum}}(y_i-w^Tx_i-b)^2+\lambda\lVert w\rVert^2$$
where $\lVert.\rVert^2$ denotes the $L_2$ norm such that $\lVert x\rVert^2=\underset{i=1}{\overset{n}{\sum}}x_i^2$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7 - inequalities}
\textbf{Chebyshev's inequality}:
$$\mathbb{P}(|Z-\mathbb{E}[Z]|>\epsilon)\le\frac{Var(Z)}{\epsilon^2}$$
\textbf{Popoviciu's inequality}:\\
Given the random variable $Z$ bounded by $a$ and $b$ such that $Z\in[a,b]$:
$$Var(Z)\le\frac{(b-a)^2}{4}$$
\textbf{Hoeffding's inequality}:\\
Given that $\hat{R}_S(h)$ is a sum of IID random variables, let $Z_1, Z_2, ..., Z_n$ be a sequence of IID random variables such that $Z\in[a,b]$ almost surely and $E[Z]=\mu$. Then:
$$\forall \epsilon > 0,\, \mathbb{P}(|\frac{1}{n}\underset{i=1}{\overset{n}{\sum}}Z_i-\mu|>\epsilon)\le2e^{\frac{-2n\epsilon^2}{(b-a)^2}}$$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8 - Linear function setup}
We consider the following environment: 
\begin{itemize}
    \item $\mathcal{X}=\mathbb{R}^d,\,\mathcal{Y}=\mathbb{R}$ with $x_i=(x_{i,1},...,x_{i,d})^T$
    \item $\{h:x\rightarrow w^Tx,\,w\in\mathbb{R}^d\}$, considering no bias term
    \item $\langle u,v\rangle = u^Tv=\underset{i=1}{\overset{d}{\sum}}u_iv_i$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \printbibliography

\end{document}