{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction number of stars for a review\n",
    "\n",
    "Our dataset is quite large, about 6GB. For debugging our code, we will use [sample](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=sample#pyspark.sql.DataFrame.sample) after reading the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_on_hdfs = \"/user/qlr/data/yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|wpSdYFSlHsW0h7pf4...|   1|2019-05-05 21:04:23|    1|Cccg84KTxjxp4iNBh...|  1.0|I scheduled a tim...|     1|qhHFB4StIDSxbbQvA...|\n",
      "|sw0nkPQvtxLtTyRnr...|   1|2017-11-21 01:28:35|    0|UK6VRt5wsx1UkNe7Y...|  3.0|Service was good-...|     0|6xfUgjxHs7XZGwmwb...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(reviews_on_hdfs).sample(0.000001)\n",
    "reviews.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Data\n",
    "\n",
    "Spark has a vast library of feature engineering functions. For example, we can get TF-IDF representation for our review corpus. In the following snippet we construct a data preparation pipeline with three stages:\n",
    "1. we get review text parsed into words\n",
    "1. we count term frequencies of our bags of words\n",
    "1. we normalise by inverted document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|      term_frequency|           embedding|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|I scheduled a tim...|[i, scheduled, a,...|(262144,[4081,791...|(262144,[4081,791...|\n",
      "|Service was good-...|[service, was, go...|(262144,[9639,156...|(262144,[9639,156...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "CPU times: user 56.1 ms, sys: 17.8 ms, total: 73.9 ms\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "data_preparation = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    HashingTF(inputCol=\"words\", outputCol=\"term_frequency\"),\n",
    "    IDF(inputCol=\"term_frequency\", outputCol=\"embedding\")\n",
    "])\n",
    "prepared_reviews = data_preparation.fit(reviews).transform(reviews)\n",
    "prepared_reviews.select(\"text\", \"words\", \"term_frequency\", \"embedding\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the details of the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text=u\"I scheduled a time for them to move my belongings and asked that they call or text when they were in route. They didn't call until they were 3 hours late. They stated their system was down and they could do it the next day instead. That was not an option for me. I only had a small quantity of boxes and a few large items and had to ask them to please come. They agreed to another time slot, for which, they were late again. Once they arrived they whipped out a contract and listed a price 4xs greater than what we agreed on.  No negotiating with them. They asked for over $400 to move less than 20 small and medium boxes a bed n couch 2 miles away. Totally unprofessional, I do not recommend, awful experience had me in tears because I waited all day for them just to pull the bait n switch on me. It was 9 pm before they finally left\", words=[u'i', u'scheduled', u'a', u'time', u'for', u'them', u'to', u'move', u'my', u'belongings', u'and', u'asked', u'that', u'they', u'call', u'or', u'text', u'when', u'they', u'were', u'in', u'route.', u'they', u\"didn't\", u'call', u'until', u'they', u'were', u'3', u'hours', u'late.', u'they', u'stated', u'their', u'system', u'was', u'down', u'and', u'they', u'could', u'do', u'it', u'the', u'next', u'day', u'instead.', u'that', u'was', u'not', u'an', u'option', u'for', u'me.', u'i', u'only', u'had', u'a', u'small', u'quantity', u'of', u'boxes', u'and', u'a', u'few', u'large', u'items', u'and', u'had', u'to', u'ask', u'them', u'to', u'please', u'come.', u'they', u'agreed', u'to', u'another', u'time', u'slot,', u'for', u'which,', u'they', u'were', u'late', u'again.', u'once', u'they', u'arrived', u'they', u'whipped', u'out', u'a', u'contract', u'and', u'listed', u'a', u'price', u'4xs', u'greater', u'than', u'what', u'we', u'agreed', u'on.', u'', u'no', u'negotiating', u'with', u'them.', u'they', u'asked', u'for', u'over', u'$400', u'to', u'move', u'less', u'than', u'20', u'small', u'and', u'medium', u'boxes', u'a', u'bed', u'n', u'couch', u'2', u'miles', u'away.', u'totally', u'unprofessional,', u'i', u'do', u'not', u'recommend,', u'awful', u'experience', u'had', u'me', u'in', u'tears', u'because', u'i', u'waited', u'all', u'day', u'for', u'them', u'just', u'to', u'pull', u'the', u'bait', u'n', u'switch', u'on', u'me.', u'it', u'was', u'9', u'pm', u'before', u'they', u'finally', u'left'], term_frequency=SparseVector(262144, {4081: 1.0, 7917: 1.0, 9639: 1.0, 12072: 1.0, 12084: 1.0, 13957: 2.0, 16332: 5.0, 19303: 1.0, 24417: 4.0, 24980: 1.0, 25217: 1.0, 25551: 1.0, 25570: 3.0, 34116: 3.0, 34182: 1.0, 37470: 2.0, 37852: 1.0, 42906: 1.0, 48448: 2.0, 49065: 1.0, 50581: 1.0, 54961: 1.0, 56407: 2.0, 63409: 1.0, 68727: 1.0, 78295: 1.0, 80112: 2.0, 81566: 1.0, 83656: 1.0, 86175: 2.0, 87603: 1.0, 87730: 1.0, 89074: 1.0, 89663: 1.0, 91677: 6.0, 92076: 1.0, 92305: 1.0, 97171: 1.0, 100258: 1.0, 100743: 2.0, 103838: 2.0, 104877: 1.0, 105627: 1.0, 108097: 1.0, 109706: 2.0, 109810: 1.0, 117924: 1.0, 120667: 1.0, 121517: 2.0, 122925: 1.0, 122945: 1.0, 124341: 1.0, 125133: 3.0, 126466: 1.0, 127390: 1.0, 135560: 1.0, 139098: 2.0, 140390: 1.0, 143985: 1.0, 147489: 1.0, 147765: 1.0, 151536: 12.0, 153779: 1.0, 156250: 1.0, 159066: 2.0, 160141: 2.0, 160916: 1.0, 164666: 1.0, 169897: 1.0, 170414: 1.0, 172787: 1.0, 175665: 1.0, 178387: 1.0, 179832: 1.0, 181635: 1.0, 186060: 1.0, 191459: 1.0, 192221: 1.0, 192301: 1.0, 193347: 1.0, 194536: 1.0, 198089: 1.0, 202715: 1.0, 203193: 1.0, 205044: 6.0, 206116: 2.0, 212053: 1.0, 212683: 1.0, 214670: 1.0, 215221: 1.0, 217873: 1.0, 218391: 1.0, 219621: 1.0, 221047: 1.0, 222069: 2.0, 222453: 2.0, 227410: 6.0, 227662: 2.0, 229898: 1.0, 230286: 1.0, 230921: 1.0, 233381: 1.0, 247107: 1.0, 249180: 1.0, 250802: 1.0, 252759: 1.0, 252843: 1.0, 253170: 3.0, 260659: 1.0}), embedding=SparseVector(262144, {4081: 0.4055, 7917: 0.4055, 9639: 0.0, 12072: 0.4055, 12084: 0.4055, 13957: 0.8109, 16332: 2.0273, 19303: 0.4055, 24417: 0.0, 24980: 0.4055, 25217: 0.4055, 25551: 0.4055, 25570: 0.0, 34116: 0.0, 34182: 0.4055, 37470: 0.8109, 37852: 0.4055, 42906: 0.4055, 48448: 0.8109, 49065: 0.4055, 50581: 0.4055, 54961: 0.4055, 56407: 0.8109, 63409: 0.4055, 68727: 0.4055, 78295: 0.4055, 80112: 0.8109, 81566: 0.4055, 83656: 0.4055, 86175: 0.8109, 87603: 0.4055, 87730: 0.4055, 89074: 0.4055, 89663: 0.4055, 91677: 2.4328, 92076: 0.4055, 92305: 0.4055, 97171: 0.4055, 100258: 0.0, 100743: 0.8109, 103838: 0.0, 104877: 0.4055, 105627: 0.4055, 108097: 0.4055, 109706: 0.8109, 109810: 0.4055, 117924: 0.4055, 120667: 0.4055, 121517: 0.8109, 122925: 0.4055, 122945: 0.4055, 124341: 0.4055, 125133: 0.0, 126466: 0.4055, 127390: 0.4055, 135560: 0.4055, 139098: 0.0, 140390: 0.4055, 143985: 0.4055, 147489: 0.4055, 147765: 0.4055, 151536: 4.8656, 153779: 0.4055, 156250: 0.4055, 159066: 0.8109, 160141: 0.8109, 160916: 0.4055, 164666: 0.4055, 169897: 0.4055, 170414: 0.4055, 172787: 0.4055, 175665: 0.4055, 178387: 0.4055, 179832: 0.4055, 181635: 0.4055, 186060: 0.4055, 191459: 0.4055, 192221: 0.4055, 192301: 0.4055, 193347: 0.4055, 194536: 0.4055, 198089: 0.4055, 202715: 0.4055, 203193: 0.4055, 205044: 2.4328, 206116: 0.8109, 212053: 0.4055, 212683: 0.4055, 214670: 0.4055, 215221: 0.4055, 217873: 0.4055, 218391: 0.4055, 219621: 0.4055, 221047: 0.4055, 222069: 0.8109, 222453: 0.8109, 227410: 2.4328, 227662: 0.8109, 229898: 0.4055, 230286: 0.4055, 230921: 0.4055, 233381: 0.4055, 247107: 0.4055, 249180: 0.4055, 250802: 0.4055, 252759: 0.4055, 252843: 0.4055, 253170: 1.2164, 260659: 0.4055}))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_reviews.select(\"text\", \"words\", \"term_frequency\", \"embedding\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the representation of TF-IDF vectors - it's sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "\n",
    "Try to follow [a tutorial from Spark docs](http://spark.apache.org/docs/latest/ml-classification-regression.html#regression)\n",
    "\n",
    "* calculate `word2vec` embeddings instead of TF-IDF\n",
    "* build a linear regression (predict stars by text)\n",
    "* split data into train and validation sets and evaluate your model\n",
    "* compare quality of models (TF-IDF vs word2vec, linear vs random forest vs gradient goosted trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|               words|               model|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|I scheduled a tim...|[i, scheduled, a,...|[7.03314652465685...|\n",
      "|Service was good-...|[service, was, go...|[4.08442619137783...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_preparation = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    Word2Vec(inputCol=\"words\", outputCol=\"model\")\n",
    "])\n",
    "prepared_reviews = data_preparation.fit(reviews).transform(reviews)\n",
    "prepared_reviews.select(\"text\", \"words\", \"model\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
