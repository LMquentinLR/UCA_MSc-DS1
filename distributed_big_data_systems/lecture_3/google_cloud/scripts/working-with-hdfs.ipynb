{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETTY_NAME=\"Debian GNU/Linux 10 (buster)\"\r\n",
      "NAME=\"Debian GNU/Linux\"\r\n",
      "VERSION_ID=\"10\"\r\n",
      "VERSION=\"10 (buster)\"\r\n",
      "VERSION_CODENAME=buster\r\n",
      "ID=debian\r\n",
      "HOME_URL=\"https://www.debian.org/\"\r\n",
      "SUPPORT_URL=\"https://www.debian.org/support\"\r\n",
      "BUG_REPORT_URL=\"https://bugs.debian.org/\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.14 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 2.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop version | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with folders on HDFS\n",
    "It's quite similar to working with folders on any \\*nix system, just don't forget to add `hdfs dfs -` before common commands:\n",
    "* `ls`\n",
    "* `mkdir`\n",
    "* `rmdir`\n",
    "* `mv`\n",
    "\n",
    "Usually it's a good idea to use your user's directory: `/uesr/%username%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/hbase\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/hdfs\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/hive\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/mapred\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/pig\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 13:47 /user/qlr\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/spark\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/yarn\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-11-19 13:20 /user/zookeeper\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/qlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/qlr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/qlr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/qlr/data/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 13:47 /user/qlr/data/tmp\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv /user/qlr/data/tmp /user/qlr/data/tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 13:47 /user/qlr/data/tmp1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rmdir /user/qlr/data/tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/qlr/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the difference between you local system and HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin   dev  hadoop  lib\t  lib64   lost+found  mnt  proc  run   srv  tmp  var\r\n",
      "boot  etc  home    lib32  libx32  media       opt  root  sbin  sys  usr\r\n"
     ]
    }
   ],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Found 3 items',\n",
       " 'drwx------   - mapred hadoop          0 2020-11-19 13:20 /hadoop',\n",
       " 'drwxrwxrwt   - hdfs   hadoop          0 2020-11-19 13:20 /tmp',\n",
       " 'drwxrwxrwt   - hdfs   hadoop          0 2020-11-19 13:47 /user']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 13:47 /user/qlr/data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/qlr/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading data from your local filesystem to HDFS and getting it back\n",
    "For uploading and downloading data from your HDFS cluster, an `ftp`-like parlance is used:\n",
    "* `put`\n",
    "* `get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put file:///home/user/Downloads/yelp_academic_dataset_review.json /user/qlr/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `du` command to measure the actual size of your data on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9 G  /user/qlr/data/yelp_academic_dataset_review.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h /user/qlr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/user/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /user/qlr/data/* file:///home/user/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 5.9G Nov 19 13:55 /home/user/Downloads/from_hdfs\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/user/Downloads/from_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind that since storage on HDFS is distributed, you can end up `get`-ing a bunch of files instead of one, each slice coming from a different node of the cluster.\n",
    "\n",
    "If you want to get a single file, use `getmerge` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have several HDFS clusters in your disposal, you can move data from one to another with the same pair of commands (`get`/`put`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "* run `hdfs dfs --help` and get an idea of other possible commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--help: Unknown command\r\n",
      "Usage: hadoop fs [generic options]\r\n",
      "\t[-appendToFile <localsrc> ... <dst>]\r\n",
      "\t[-cat [-ignoreCrc] <src> ...]\r\n",
      "\t[-checksum <src> ...]\r\n",
      "\t[-chgrp [-R] GROUP PATH...]\r\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\r\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\r\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\r\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]\r\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\r\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\r\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\r\n",
      "\t[-df [-h] [<path> ...]]\r\n",
      "\t[-du [-s] [-h] [-x] <path> ...]\r\n",
      "\t[-expunge]\r\n",
      "\t[-find <path> ... <expression> ...]\r\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-getfacl [-R] <path>]\r\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\r\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\r\n",
      "\t[-help [cmd ...]]\r\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\r\n",
      "\t[-mkdir [-p] <path> ...]\r\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\r\n",
      "\t[-moveToLocal <src> <localdst>]\r\n",
      "\t[-mv <src> ... <dst>]\r\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\r\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\r\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\r\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\r\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\r\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\r\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\r\n",
      "\t[-stat [format] <path> ...]\r\n",
      "\t[-tail [-f] <file>]\r\n",
      "\t[-test -[defsz] <path>]\r\n",
      "\t[-text [-ignoreCrc] <src> ...]\r\n",
      "\t[-touchz <path> ...]\r\n",
      "\t[-truncate [-w] <length> <path> ...]\r\n",
      "\t[-usage [cmd ...]]\r\n",
      "\r\n",
      "Generic options supported are:\r\n",
      "-conf <configuration file>        specify an application configuration file\r\n",
      "-D <property=value>               define a value for a given property\r\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\r\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\r\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\r\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\r\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\r\n",
      "\r\n",
      "The general command line syntax is:\r\n",
      "command [genericOptions] [commandOptions]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* try to create a subfolder in your `/user/%username%` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/qlr/test_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a new file on HDFS using `touch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -touchz /user/qlr/test_folder/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 root hadoop        139 2020-11-19 14:10 /user/qlr/test_folder/mapper.py\n",
      "-rw-r--r--   2 root hadoop          0 2020-11-19 14:04 /user/qlr/test_folder/test.py\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/test_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `cat` the newly created file (don't do that in future when working with huge files:))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/qlr/test_folder/test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* upload some files from your system to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put file:///home/user/Downloads/mapper.py /user/qlr/test_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use `head`/`tail` to look into the uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "\r\n",
      "counter = 0\r\n",
      "while True:\r\n",
      "    try:\r\n",
      "        counter += 1\r\n",
      "        input()\r\n",
      "    except EOFError:\r\n",
      "        break\r\n",
      "print(counter)\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/qlr/test_folder/mapper.py | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "\r\n",
      "counter = 0\r\n",
      "while True:\r\n",
      "    try:\r\n",
      "        counter += 1\r\n",
      "        input()\r\n",
      "    except EOFError:\r\n",
      "        break\r\n",
      "print(counter)\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /user/qlr/test_folder/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a full copy of your working subfolder on HDFS (use `cp`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp /user/qlr/test_folder /user/qlr/test_folder_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root hadoop        139 2020-11-19 14:14 /user/qlr/test_folder_copy/mapper.py\r\n",
      "-rw-r--r--   2 root hadoop          0 2020-11-19 14:14 /user/qlr/test_folder_copy/test.py\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/test_folder_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* remove the redundant copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/qlr/test_folder_copy\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /user/qlr/test_folder_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compare the results of `get` and `getmerge` commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compare the results of `put` and `moveFromLocal` commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* feel free to play with other HDFS commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "working-with-hdfs.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
