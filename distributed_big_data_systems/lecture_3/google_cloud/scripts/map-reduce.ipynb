{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming\n",
    "\n",
    "Usually MapReduce jobs are written in Java. Nevertheless, Hadoop has a feature called somewhat misleadingly [Hadoop Streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) which enables one to use Python or any other script language such as `shell` for developing mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Some Code\n",
    "\n",
    "First, we need to code our mapper. In case of Hadoop Streaming, a mapper is a script which gets some text from the standard input until the EOF and produces some text line by line to the standard output. For example, it can be like the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "\r\n",
      "counter = 0\r\n",
      "while True:\r\n",
      "    try:\r\n",
      "        counter += 1\r\n",
      "        input()\r\n",
      "    except EOFError:\r\n",
      "        break\r\n",
      "print(counter)\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/borisshminke/Downloads/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the first line (so-called [shebang](https://en.wikipedia.org/wiki/Shebang_(Unix))) - it's very important to keep it in place, since Hadoop doesn't know where your favourite Python executable is located. It could even be `#!/usr/bin/perl` or `#!/usr/bin/bash` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script does nothing interesting, it simply goes through the file line by line until EOF and counts lines. Then it prints the total number of lines in a file.\n",
    "\n",
    "Of course, you can code anything more complicated: import additional packages, define functions, classes, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer looks similar since generally it does the same trick: goes through the lines of the standard input and prints something to the standard output. The main difference is that it has the output of the mapper as it's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "\r\n",
      "counter = 0\r\n",
      "while True:\r\n",
      "    try:\r\n",
      "        line = input()\r\n",
      "    except EOFError:\r\n",
      "        break\r\n",
      "    counter += int(line)\r\n",
      "print(counter)\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/borisshminke/Downloads/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reducer sums integer values (which are the line counts produced by the mapper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing your code to the cluster\n",
    "\n",
    "Hadoop lives on a cluster, and your MapReduce jobs will run on the cluster too. Hadoop __can't__ execute any code from a local machine directly. That means we need to put out code to HDFS somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/qlr/code\n",
    "!hdfs dfs -put \\\n",
    "    file:///home/user/Downloads/mapper.py \\\n",
    "    /user/qlr/code\n",
    "!hdfs dfs -put \\\n",
    "    file:///home/user/Downloads/reducer.py \\\n",
    "    /user/qlr/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root hadoop        138 2020-11-19 14:28 /user/qlr/code/mapper.py\r\n",
      "-rw-r--r--   2 root hadoop        150 2020-11-19 14:28 /user/qlr/code/reducer.py\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to do that whenever your want to update your MapReduce jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running MapReduce\n",
    "\n",
    "We will use the `mapred streaming` command for running our Hadoop Streaming job. The description of parameters follows:\n",
    "* files - here we put a comma-separated list of our source code files __on HDFS__. In case of Python they are simple Python scripts but in case of Java they would be `jar` files\n",
    "* input - a file on HDFS to input to the mapper\n",
    "* output - some location to HDFS where to put the results (the output of the reducer)\n",
    "* mapper - the name of the mapper script\n",
    "* reducer - the name of the reducer script\n",
    "\n",
    "Let magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar] /tmp/streamjob5407524167963061102.jar tmpDir=null\n",
      "20/11/19 14:36:20 INFO client.RMProxy: Connecting to ResourceManager at cluster-big-data-uca-m/10.132.0.2:8032\n",
      "20/11/19 14:36:20 INFO client.AHSProxy: Connecting to Application History server at cluster-big-data-uca-m/10.132.0.2:10200\n",
      "20/11/19 14:36:20 INFO client.RMProxy: Connecting to ResourceManager at cluster-big-data-uca-m/10.132.0.2:8032\n",
      "20/11/19 14:36:20 INFO client.AHSProxy: Connecting to Application History server at cluster-big-data-uca-m/10.132.0.2:10200\n",
      "20/11/19 14:36:21 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "20/11/19 14:36:21 INFO mapreduce.JobSubmitter: number of splits:48\n",
      "20/11/19 14:36:21 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "20/11/19 14:36:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1605791986265_0001\n",
      "20/11/19 14:36:22 INFO impl.YarnClientImpl: Submitted application application_1605791986265_0001\n",
      "20/11/19 14:36:22 INFO mapreduce.Job: The url to track the job: http://cluster-big-data-uca-m:8088/proxy/application_1605791986265_0001/\n",
      "20/11/19 14:36:22 INFO mapreduce.Job: Running job: job_1605791986265_0001\n",
      "20/11/19 14:36:30 INFO mapreduce.Job: Job job_1605791986265_0001 running in uber mode : false\n",
      "20/11/19 14:36:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/11/19 14:36:49 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "20/11/19 14:36:52 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "20/11/19 14:36:54 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "20/11/19 14:37:06 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "20/11/19 14:37:14 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "20/11/19 14:37:15 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "20/11/19 14:37:16 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "20/11/19 14:37:23 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "20/11/19 14:37:33 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "20/11/19 14:37:35 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "20/11/19 14:37:36 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "20/11/19 14:37:37 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "20/11/19 14:37:38 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "20/11/19 14:37:40 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/11/19 14:37:52 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "20/11/19 14:37:55 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "20/11/19 14:37:56 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "20/11/19 14:37:57 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "20/11/19 14:37:58 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "20/11/19 14:38:11 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "20/11/19 14:38:13 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "20/11/19 14:38:16 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "20/11/19 14:38:17 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "20/11/19 14:38:18 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "20/11/19 14:38:19 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "20/11/19 14:38:28 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "20/11/19 14:38:32 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "20/11/19 14:38:33 INFO mapreduce.Job:  map 86% reduce 0%\n",
      "20/11/19 14:38:36 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "20/11/19 14:38:37 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "20/11/19 14:38:38 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "20/11/19 14:38:41 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "20/11/19 14:38:44 INFO mapreduce.Job:  map 98% reduce 0%\n",
      "20/11/19 14:38:49 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "20/11/19 14:38:50 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "20/11/19 14:38:52 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "20/11/19 14:38:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/11/19 14:38:53 INFO mapreduce.Job: Job job_1605791986265_0001 completed successfully\n",
      "20/11/19 14:38:53 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=521\n",
      "\t\tFILE: Number of bytes written=11640101\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6325763928\n",
      "\t\tHDFS: Number of bytes written=61\n",
      "\t\tHDFS: Number of read operations=179\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=21\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=48\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=48\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2534559\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=122253\n",
      "\t\tTotal time spent by all map tasks (ms)=844853\n",
      "\t\tTotal time spent by all reduce tasks (ms)=40751\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=844853\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=40751\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2595388416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=125187072\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8021122\n",
      "\t\tMap output records=48\n",
      "\t\tMap output bytes=383\n",
      "\t\tMap output materialized bytes=2495\n",
      "\t\tInput split bytes=6192\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=48\n",
      "\t\tReduce shuffle bytes=2495\n",
      "\t\tReduce input records=48\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=96\n",
      "\t\tShuffled Maps =336\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=336\n",
      "\t\tGC time elapsed (ms)=9254\n",
      "\t\tCPU time spent (ms)=351630\n",
      "\t\tPhysical memory (bytes) snapshot=29083217920\n",
      "\t\tVirtual memory (bytes) snapshot=240526458880\n",
      "\t\tTotal committed heap usage (bytes)=25285885952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6325757736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "20/11/19 14:38:53 INFO streaming.StreamJob: Output directory: /user/qlr/data/result\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar \\\n",
    "    -files hdfs:///user/qlr/code/mapper.py,hdfs:///user/qlr/code/reducer.py \\\n",
    "    -input /user/qlr/data/yelp_academic_dataset_review.json \\\n",
    "    -output /user/qlr/data/result \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rw-r--r--   2 root hadoop          0 2020-11-19 14:38 /user/qlr/data/result/_SUCCESS\n",
      "-rw-r--r--   2 root hadoop          8 2020-11-19 14:38 /user/qlr/data/result/part-00000\n",
      "-rw-r--r--   2 root hadoop          9 2020-11-19 14:38 /user/qlr/data/result/part-00001\n",
      "-rw-r--r--   2 root hadoop          9 2020-11-19 14:38 /user/qlr/data/result/part-00002\n",
      "-rw-r--r--   2 root hadoop          9 2020-11-19 14:38 /user/qlr/data/result/part-00003\n",
      "-rw-r--r--   2 root hadoop          9 2020-11-19 14:38 /user/qlr/data/result/part-00004\n",
      "-rw-r--r--   2 root hadoop          8 2020-11-19 14:38 /user/qlr/data/result/part-00005\n",
      "-rw-r--r--   2 root hadoop          9 2020-11-19 14:38 /user/qlr/data/result/part-00006\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/data/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the file \\_SUCCESS. It appears only when a MapReduce job finished successfully. Since the reducer outputs files, here they are - `part-0000*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019785\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/qlr/data/result/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682511\t\r\n",
      "1022698\t\r\n",
      "1019785\t\r\n",
      "1517790\t\r\n",
      "1220647\t\r\n",
      "838812\t\r\n",
      "1718927\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/qlr/data/result/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8021122 /home/user/Downloads/yelp_academic_dataset_review.json\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /home/user/Downloads/yelp_academic_dataset_review.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the numbers of lines in our input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That happened because our mapper was run in parallel. You can wonder why it was slow then. The answer is quite simple - Python is an interpreted language, so it would be faster if we were using C++ or Java for our mappers and reducers.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Removing erroneous folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 14:38 /user/qlr/data/result\r\n",
      "-rw-r--r--   2 root hadoop 6325565224 2020-11-19 13:54 /user/qlr/data/yelp_academic_dataset_review.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/qlr/test_folder\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /user/qlr/test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 14:28 /user/qlr/code\r\n",
      "drwxr-xr-x   - root hadoop          0 2020-11-19 14:36 /user/qlr/data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "* code a mapper for counting characters, not lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"xQY8N_XvtGbearJ5X4QryQ\",\"user_id\":\"OwjRMXRC0KyPrIlcjaXeFQ\",\"business_id\":\"-MhfebM0QIsKt87iDN-FNw\",\"stars\":2.0,\"useful\":5,\"funny\":0,\"cool\":0,\"text\":\"As someone who has worked with many museums, I was eager to visit this gallery on my most recent trip to Las Vegas. When I saw they would be showing infamous eggs of the House of Faberge from the Virginia Museum of Fine Arts (VMFA), I knew I had to go!\\n\\nTucked away near the gelateria and the garden, the Gallery is pretty much hidden from view. It's what real estate agents would call \\\"cozy\\\" or \\\"charming\\\" - basically any euphemism for small.\\n\\nThat being said, you can still see wonderful art at a gallery of any size, so why the two *s you ask? Let me tell you:\\n\\n* pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top. For the space and the amount of art you can fit in there, it is a bit much.\\n* it's not kid friendly at all. Seriously, don't bring them.\\n* the security is not trained properly for the show. When the curating and design teams collaborate for exhibitions, there is a definite flow. That means visitors should view the art in a certain sequence, whether it be by historical period or cultural significance (this is how audio guides are usually developed). When I arrived in the gallery I could not tell where to start, and security was certainly not helpful. I was told to \\\"just look around\\\" and \\\"do whatever.\\\" \\n\\nAt such a *fine* institution, I find the lack of knowledge and respect for the art appalling.\",\"date\":\"2015-04-15 05:21:16\"}\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/qlr/data/yelp_academic_dataset_review.json | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put \\\n",
    "    file:///home/user/Downloads/mapper-char.py \\\n",
    "    /user/qlr/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   2 root hadoop        151 2020-11-19 14:52 /user/qlr/code/mapper-char.py\r\n",
      "-rw-r--r--   2 root hadoop        138 2020-11-19 14:28 /user/qlr/code/mapper.py\r\n",
      "-rw-r--r--   2 root hadoop        150 2020-11-19 14:28 /user/qlr/code/reducer.py\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar] /tmp/streamjob2451061377900387608.jar tmpDir=null\n",
      "20/11/19 14:53:12 INFO client.RMProxy: Connecting to ResourceManager at cluster-big-data-uca-m/10.132.0.2:8032\n",
      "20/11/19 14:53:12 INFO client.AHSProxy: Connecting to Application History server at cluster-big-data-uca-m/10.132.0.2:10200\n",
      "20/11/19 14:53:12 INFO client.RMProxy: Connecting to ResourceManager at cluster-big-data-uca-m/10.132.0.2:8032\n",
      "20/11/19 14:53:12 INFO client.AHSProxy: Connecting to Application History server at cluster-big-data-uca-m/10.132.0.2:10200\n",
      "20/11/19 14:53:12 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "20/11/19 14:53:12 INFO mapreduce.JobSubmitter: number of splits:48\n",
      "20/11/19 14:53:13 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "20/11/19 14:53:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1605791986265_0002\n",
      "20/11/19 14:53:13 INFO impl.YarnClientImpl: Submitted application application_1605791986265_0002\n",
      "20/11/19 14:53:13 INFO mapreduce.Job: The url to track the job: http://cluster-big-data-uca-m:8088/proxy/application_1605791986265_0002/\n",
      "20/11/19 14:53:13 INFO mapreduce.Job: Running job: job_1605791986265_0002\n",
      "20/11/19 14:53:19 INFO mapreduce.Job: Job job_1605791986265_0002 running in uber mode : false\n",
      "20/11/19 14:53:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/11/19 14:53:36 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "20/11/19 14:53:37 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "20/11/19 14:53:40 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "20/11/19 14:53:41 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "20/11/19 14:53:42 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "20/11/19 14:53:54 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "20/11/19 14:53:55 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "20/11/19 14:54:02 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "20/11/19 14:54:04 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "20/11/19 14:54:12 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "20/11/19 14:54:23 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "20/11/19 14:54:24 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "20/11/19 14:54:25 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "20/11/19 14:54:28 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "20/11/19 14:54:29 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/11/19 14:54:43 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "20/11/19 14:54:44 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "20/11/19 14:54:45 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "20/11/19 14:54:46 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "20/11/19 14:55:02 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "20/11/19 14:55:03 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "20/11/19 14:55:04 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "20/11/19 14:55:05 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "20/11/19 14:55:06 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "20/11/19 14:55:07 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "20/11/19 14:55:20 INFO mapreduce.Job:  map 85% reduce 0%\n",
      "20/11/19 14:55:23 INFO mapreduce.Job:  map 86% reduce 0%\n",
      "20/11/19 14:55:26 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "20/11/19 14:55:27 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "20/11/19 14:55:28 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "20/11/19 14:55:29 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "20/11/19 14:55:35 INFO mapreduce.Job:  map 98% reduce 0%\n",
      "20/11/19 14:55:36 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "20/11/19 14:55:37 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "20/11/19 14:55:38 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "20/11/19 14:55:40 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "20/11/19 14:55:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/11/19 14:55:41 INFO mapreduce.Job: Job job_1605791986265_0002 completed successfully\n",
      "20/11/19 14:55:41 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=569\n",
      "\t\tFILE: Number of bytes written=11641902\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6325763928\n",
      "\t\tHDFS: Number of bytes written=66\n",
      "\t\tHDFS: Number of read operations=179\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=21\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=48\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=48\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2573694\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=107952\n",
      "\t\tTotal time spent by all map tasks (ms)=857898\n",
      "\t\tTotal time spent by all reduce tasks (ms)=35984\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=857898\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=35984\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2635462656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=110542848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8021122\n",
      "\t\tMap output records=48\n",
      "\t\tMap output bytes=431\n",
      "\t\tMap output materialized bytes=2543\n",
      "\t\tInput split bytes=6192\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=48\n",
      "\t\tReduce shuffle bytes=2543\n",
      "\t\tReduce input records=48\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=96\n",
      "\t\tShuffled Maps =336\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=336\n",
      "\t\tGC time elapsed (ms)=9324\n",
      "\t\tCPU time spent (ms)=396990\n",
      "\t\tPhysical memory (bytes) snapshot=29236158464\n",
      "\t\tVirtual memory (bytes) snapshot=240410824704\n",
      "\t\tTotal committed heap usage (bytes)=25432162304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6325757736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=66\n",
      "20/11/19 14:55:41 INFO streaming.StreamJob: Output directory: /user/qlr/data/result_char_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar \\\n",
    "    -files hdfs:///user/qlr/code/mapper-char.py,hdfs:///user/qlr/code/reducer.py \\\n",
    "    -input /user/qlr/data/yelp_academic_dataset_review.json \\\n",
    "    -output /user/qlr/data/result_char_count \\\n",
    "    -mapper mapper-char.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10797192\t\r\n",
      "9204228\t\r\n",
      "7549263\t\r\n",
      "9178011\t\r\n",
      "15470253\t\r\n",
      "13660029\t\r\n",
      "6331122\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/qlr/data/result_char_count/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* code a reducer to count lines and characters in one job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/qlr/data/result_char_lines_count\n",
      "Deleted /user/qlr/code/mapper-char-lines.py\n",
      "Deleted /user/qlr/code/reducer-char-lines.py\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /user/qlr/data/result_char_lines_count\n",
    "!hdfs dfs -rm -R /user/qlr/code/mapper-char-lines.py\n",
    "!hdfs dfs -rm -R /user/qlr/code/reducer-char-lines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put \\\n",
    "    file:///home/user/Downloads/mapper-char-lines.py \\\n",
    "    /user/qlr/code\n",
    "!hdfs dfs -put \\\n",
    "    file:///home/user/Downloads/reducer-char-lines.py \\\n",
    "    /user/qlr/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   2 root hadoop        210 2020-11-19 15:20 /user/qlr/code/mapper-char-lines.py\r\n",
      "-rw-r--r--   2 root hadoop        151 2020-11-19 14:52 /user/qlr/code/mapper-char.py\r\n",
      "-rw-r--r--   2 root hadoop        138 2020-11-19 14:28 /user/qlr/code/mapper.py\r\n",
      "-rw-r--r--   2 root hadoop        314 2020-11-19 15:20 /user/qlr/code/reducer-char-lines.py\r\n",
      "-rw-r--r--   2 root hadoop        150 2020-11-19 14:28 /user/qlr/code/reducer.py\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/qlr/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar] /tmp/streamjob8127088355966363102.jar tmpDir=null\n",
      "20/11/19 15:20:55 INFO client.RMProxy: Connecting to ResourceManager at cluster-big-data-uca-m/10.132.0.2:8032\n",
      "20/11/19 15:20:55 INFO client.AHSProxy: Connecting to Application History server at cluster-big-data-uca-m/10.132.0.2:10200\n",
      "20/11/19 15:20:55 INFO client.RMProxy: Connecting to ResourceManager at cluster-big-data-uca-m/10.132.0.2:8032\n",
      "20/11/19 15:20:55 INFO client.AHSProxy: Connecting to Application History server at cluster-big-data-uca-m/10.132.0.2:10200\n",
      "20/11/19 15:20:55 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "20/11/19 15:20:55 INFO mapreduce.JobSubmitter: number of splits:48\n",
      "20/11/19 15:20:55 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "20/11/19 15:20:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1605791986265_0008\n",
      "20/11/19 15:20:56 INFO impl.YarnClientImpl: Submitted application application_1605791986265_0008\n",
      "20/11/19 15:20:56 INFO mapreduce.Job: The url to track the job: http://cluster-big-data-uca-m:8088/proxy/application_1605791986265_0008/\n",
      "20/11/19 15:20:56 INFO mapreduce.Job: Running job: job_1605791986265_0008\n",
      "20/11/19 15:21:02 INFO mapreduce.Job: Job job_1605791986265_0008 running in uber mode : false\n",
      "20/11/19 15:21:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/11/19 15:21:20 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "20/11/19 15:21:21 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "20/11/19 15:21:22 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "20/11/19 15:21:23 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "20/11/19 15:21:36 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "20/11/19 15:21:38 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "20/11/19 15:21:40 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "20/11/19 15:21:41 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "20/11/19 15:21:52 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "20/11/19 15:21:53 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "20/11/19 15:21:55 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "20/11/19 15:22:00 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "20/11/19 15:22:01 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "20/11/19 15:22:08 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "20/11/19 15:22:09 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "20/11/19 15:22:11 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/11/19 15:22:20 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "20/11/19 15:22:21 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "20/11/19 15:22:25 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "20/11/19 15:22:26 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "20/11/19 15:22:38 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "20/11/19 15:22:40 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "20/11/19 15:22:41 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "20/11/19 15:22:42 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "20/11/19 15:22:44 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "20/11/19 15:22:45 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "20/11/19 15:22:57 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "20/11/19 15:22:58 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "20/11/19 15:22:59 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "20/11/19 15:23:01 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "20/11/19 15:23:04 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "20/11/19 15:23:13 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "20/11/19 15:23:14 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "20/11/19 15:23:17 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "20/11/19 15:23:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/11/19 15:23:18 INFO mapreduce.Job: Job job_1605791986265_0008 completed successfully\n",
      "20/11/19 15:23:18 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=904\n",
      "\t\tFILE: Number of bytes written=11646642\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6325763928\n",
      "\t\tHDFS: Number of bytes written=402\n",
      "\t\tHDFS: Number of read operations=179\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=21\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=48\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=48\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2367261\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=126978\n",
      "\t\tTotal time spent by all map tasks (ms)=789087\n",
      "\t\tTotal time spent by all reduce tasks (ms)=42326\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=789087\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=42326\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2424075264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=130025472\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8021122\n",
      "\t\tMap output records=48\n",
      "\t\tMap output bytes=766\n",
      "\t\tMap output materialized bytes=2878\n",
      "\t\tInput split bytes=6192\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=48\n",
      "\t\tReduce shuffle bytes=2878\n",
      "\t\tReduce input records=48\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=96\n",
      "\t\tShuffled Maps =336\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=336\n",
      "\t\tGC time elapsed (ms)=8532\n",
      "\t\tCPU time spent (ms)=337360\n",
      "\t\tPhysical memory (bytes) snapshot=29305057280\n",
      "\t\tVirtual memory (bytes) snapshot=240127946752\n",
      "\t\tTotal committed heap usage (bytes)=25341460480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6325757736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=402\n",
      "20/11/19 15:23:18 INFO streaming.StreamJob: Output directory: /user/qlr/data/result_char_lines_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.9.2.jar \\\n",
    "    -files hdfs:///user/qlr/code/mapper-char-lines.py,hdfs:///user/qlr/code/reducer-char-lines.py \\\n",
    "    -input /user/qlr/data/yelp_academic_dataset_review.json \\\n",
    "    -output /user/qlr/data/result_char_lines_count \\\n",
    "    -mapper mapper-char-lines.py \\\n",
    "    -reducer reducer-char-lines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines: 1528677; number of characters: 13758012\t\r\n",
      "number of lines: 1368722; number of characters: 12318426\t\r\n",
      "number of lines: 1195213; number of characters: 10756854\t\r\n",
      "number of lines: 1201206; number of characters: 10810791\t\r\n",
      "number of lines: 1011019; number of characters: 9099117\t\r\n",
      "number of lines: 1016714; number of characters: 9150372\t\r\n",
      "number of lines: 699619; number of characters: 6296526\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/qlr/data/result_char_lines_count/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* code a mapper to sum over the `compliment_count` field value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "map-reduce.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
