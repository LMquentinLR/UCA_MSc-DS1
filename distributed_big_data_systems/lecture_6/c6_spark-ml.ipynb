{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://yelp-dataset-bucket/yelp_academic_dataset_review.json...\n",
      "- [1 files][  5.9 GiB/  5.9 GiB]   50.8 MiB/s                                   \n",
      "Operation completed over 1 objects/5.9 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://yelp-dataset-bucket/yelp_academic_dataset_review.json /home/user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/user/yelp_academic_dataset_review.json /user/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction number of stars for a review\n",
    "\n",
    "Our dataset is quite large, about 6GB. For debugging our code, we will use [sample](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=sample#pyspark.sql.DataFrame.sample) after reading the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_on_hdfs = \"/user/data/yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|-lCSC0-seRf1KZUeL...|   0|2011-12-19 20:18:57|    0|7eUgRTX-Y5Fudkgod...|  4.0|My favourite room...|     0|HFItzRohDHZvcKDrM...|\n",
      "|KEaCHdsY7w7CBsZ6h...|   0|2007-10-13 14:13:32|    0|FbI1y7TPgnQd_xCL_...|  4.0|We went to the Gr...|     0|d0LUROgBb3R5eAEio...|\n",
      "|FCP5hYaTtn6dkpmZ_...|   0|2010-07-04 09:25:51|    0|KAqyziQ_VXNlgRDiL...|  4.0|1st, not the best...|     0|ijExLQtBHr4FXb3yf...|\n",
      "|p8HvhJZ-_EHhmUVmZ...|   0|2014-06-26 03:05:47|    0|Az0KZ1E4GXmS7PJ-Z...|  4.0|This place was aw...|     0|BNosARG4V6JBJlXe0...|\n",
      "|faPVqws-x-5k2CQKD...|   0|2018-04-07 18:16:21|    0|kEwQ2ljpqTpPnJIbQ...|  4.0|It is good.  The ...|     0|aubu0kBLsiH1oYiC_...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(reviews_on_hdfs).sample(0.000001)\n",
    "reviews.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Data\n",
    "\n",
    "Spark has a vast library of feature engineering functions. For example, we can get TF-IDF representation for our review corpus. In the following snippet we construct a data preparation pipeline with three stages:\n",
    "1. we get review text parsed into words\n",
    "1. we count term frequencies of our bags of words\n",
    "1. we normalise by inverted document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|      term_frequency|           embedding|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|My favourite room...|[my, favourite, r...|(262144,[2916,538...|(262144,[2916,538...|\n",
      "|We went to the Gr...|[we, went, to, th...|(262144,[3340,538...|(262144,[3340,538...|\n",
      "|1st, not the best...|[1st,, not, the, ...|(262144,[14,2315,...|(262144,[14,2315,...|\n",
      "|This place was aw...|[this, place, was...|(262144,[5630,156...|(262144,[5630,156...|\n",
      "|It is good.  The ...|[it, is, good., ,...|(262144,[9639,158...|(262144,[9639,158...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 95.4 ms, sys: 33.5 ms, total: 129 ms\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "data_preparation = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    HashingTF(inputCol=\"words\", outputCol=\"term_frequency\"),\n",
    "    IDF(inputCol=\"term_frequency\", outputCol=\"embedding\")\n",
    "])\n",
    "prepared_reviews = data_preparation.fit(reviews).transform(reviews)\n",
    "prepared_reviews.select(\"text\", \"words\", \"term_frequency\", \"embedding\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the details of the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text=u\"My favourite room in the house (other than my bedroom because I sleep like a champ), is the kitchen!  \\n\\nThey have a lot of neat stuff.  I prefer to come here for all my kitchen essentials because the stuff they have is of good quality, and its trendy and modern.  The store is pretty easy to navigate through.  \\n\\nThe prices are quite good, especially around the holidays and unlike some other places (Home Outfitters, Homesense), you can browse their product offerings online to cut down on time wasted in the store.  They will even tell you if something is in stock (not down to the very last one, but that's good enough for me!).\", words=[u'my', u'favourite', u'room', u'in', u'the', u'house', u'(other', u'than', u'my', u'bedroom', u'because', u'i', u'sleep', u'like', u'a', u'champ),', u'is', u'the', u'kitchen!', u'', u'', u'', u'they', u'have', u'a', u'lot', u'of', u'neat', u'stuff.', u'', u'i', u'prefer', u'to', u'come', u'here', u'for', u'all', u'my', u'kitchen', u'essentials', u'because', u'the', u'stuff', u'they', u'have', u'is', u'of', u'good', u'quality,', u'and', u'its', u'trendy', u'and', u'modern.', u'', u'the', u'store', u'is', u'pretty', u'easy', u'to', u'navigate', u'through.', u'', u'', u'', u'the', u'prices', u'are', u'quite', u'good,', u'especially', u'around', u'the', u'holidays', u'and', u'unlike', u'some', u'other', u'places', u'(home', u'outfitters,', u'homesense),', u'you', u'can', u'browse', u'their', u'product', u'offerings', u'online', u'to', u'cut', u'down', u'on', u'time', u'wasted', u'in', u'the', u'store.', u'', u'they', u'will', u'even', u'tell', u'you', u'if', u'something', u'is', u'in', u'stock', u'(not', u'down', u'to', u'the', u'very', u'last', u'one,', u'but', u\"that's\", u'good', u'enough', u'for', u'me!).'], term_frequency=SparseVector(262144, {2916: 1.0, 5381: 1.0, 7367: 1.0, 9639: 2.0, 11451: 1.0, 15889: 4.0, 16332: 2.0, 19136: 1.0, 19208: 1.0, 21032: 1.0, 24417: 2.0, 25217: 2.0, 33209: 1.0, 36073: 1.0, 37852: 3.0, 39081: 1.0, 47032: 1.0, 50528: 1.0, 51149: 1.0, 54477: 1.0, 55242: 1.0, 63050: 1.0, 63689: 1.0, 78329: 1.0, 81008: 1.0, 84377: 1.0, 85530: 1.0, 86978: 1.0, 89356: 1.0, 90809: 1.0, 91006: 1.0, 91677: 3.0, 94853: 1.0, 96984: 1.0, 99346: 1.0, 100258: 1.0, 100604: 1.0, 103838: 8.0, 113432: 2.0, 120904: 1.0, 121517: 1.0, 122925: 2.0, 128231: 1.0, 129154: 1.0, 135560: 1.0, 136006: 1.0, 137990: 1.0, 145509: 1.0, 150319: 1.0, 151536: 3.0, 160141: 1.0, 164686: 1.0, 165159: 1.0, 167122: 1.0, 169790: 1.0, 173297: 1.0, 174966: 1.0, 175449: 1.0, 183237: 1.0, 188737: 1.0, 189683: 1.0, 190138: 1.0, 199452: 1.0, 204380: 1.0, 205044: 4.0, 207779: 1.0, 208258: 1.0, 210040: 1.0, 212683: 1.0, 212733: 1.0, 215992: 1.0, 216655: 1.0, 222186: 1.0, 222453: 3.0, 223279: 1.0, 223487: 1.0, 225577: 1.0, 227410: 2.0, 230367: 1.0, 236905: 1.0, 244330: 1.0, 249180: 9.0, 251396: 1.0, 252801: 2.0, 253475: 2.0}), embedding=SparseVector(262144, {2916: 1.8458, 5381: 1.5581, 7367: 1.5581, 9639: 0.6108, 11451: 2.2513, 15889: 1.518, 16332: 1.2837, 19136: 2.2513, 19208: 1.5581, 21032: 2.2513, 24417: 0.6108, 25217: 3.6917, 33209: 2.2513, 36073: 1.335, 37852: 2.595, 39081: 1.5581, 47032: 1.5581, 50528: 2.2513, 51149: 2.2513, 54477: 2.2513, 55242: 1.1527, 63050: 2.2513, 63689: 2.2513, 78329: 1.8458, 81008: 1.8458, 84377: 2.2513, 85530: 2.2513, 86978: 2.2513, 89356: 1.1527, 90809: 2.2513, 91006: 2.2513, 91677: 0.5156, 94853: 2.2513, 96984: 2.2513, 99346: 1.8458, 100258: 0.865, 100604: 1.8458, 103838: 0.8898, 113432: 1.9971, 120904: 1.8458, 121517: 1.5581, 122925: 2.67, 128231: 1.8458, 129154: 2.2513, 135560: 1.5581, 136006: 2.2513, 137990: 2.2513, 145509: 2.2513, 150319: 1.1527, 151536: 2.595, 160141: 1.335, 164686: 2.2513, 165159: 2.2513, 167122: 0.6419, 169790: 2.2513, 173297: 1.8458, 174966: 1.5581, 175449: 1.5581, 183237: 1.8458, 188737: 2.2513, 189683: 1.1527, 190138: 2.2513, 199452: 2.2513, 204380: 1.1527, 205044: 1.2215, 207779: 2.2513, 208258: 0.9985, 210040: 0.7472, 212683: 0.9985, 212733: 2.2513, 215992: 2.2513, 216655: 2.2513, 222186: 2.2513, 222453: 2.9956, 223279: 2.2513, 223487: 2.2513, 225577: 2.2513, 227410: 0.759, 230367: 1.8458, 236905: 2.2513, 244330: 2.2513, 249180: 2.7484, 251396: 2.2513, 252801: 2.3054, 253475: 1.9971}))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_reviews.select(\"text\", \"words\", \"term_frequency\", \"embedding\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the representation of TF-IDF vectors - it's sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "\n",
    "Try to follow [a tutorial from Spark docs](http://spark.apache.org/docs/latest/ml-classification-regression.html#regression)\n",
    "\n",
    "* calculate `word2vec` embeddings instead of TF-IDF\n",
    "* build a linear regression (predict stars by text)\n",
    "* split data into train and validation sets and evaluate your model\n",
    "* compare quality of models (TF-IDF vs word2vec, linear vs random forest vs gradient goosted trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|               words|               model|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|My favourite room...|[my, favourite, r...|[0.00131861418102...|\n",
      "|We went to the Gr...|[we, went, to, th...|[6.01509346811593...|\n",
      "|1st, not the best...|[1st,, not, the, ...|[0.00119749398695...|\n",
      "|This place was aw...|[this, place, was...|[6.51844831882044...|\n",
      "|It is good.  The ...|[it, is, good., ,...|[0.00146789905875...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1237.4554721391437,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-431.49024523906286,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1077.5112016550845,0.0,0.0,-929.5960582214364,0.0,0.0,0.0,0.0,0.0,0.0,1002.9030688934004,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-131.79415085627096,557.871876985645,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,174.03667845517214,-602.5760199759625,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1215.1351089513748,0.0,0.0,0.0,0.0]\n",
      "intercept: 5.81066497811\n"
     ]
    }
   ],
   "source": [
    "data_preparation = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    Word2Vec(inputCol=\"words\", outputCol=\"model\")\n",
    "])\n",
    "\n",
    "prepared_reviews = data_preparation.fit(reviews).transform(reviews)\n",
    "prepared_reviews.select(\"text\", \"words\", \"model\").show(n=5)\n",
    "\n",
    "regression = LinearRegression(featuresCol=\"model\", labelCol=\"stars\", maxIter=100, regParam=0.2, elasticNetParam=0.5)\n",
    "model = regression.fit(prepared_reviews)\n",
    "\n",
    "print(\"coefficients: \" + str(model.coefficients))\n",
    "print(\"intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business_id',\n",
       " 'cool',\n",
       " 'date',\n",
       " 'funny',\n",
       " 'review_id',\n",
       " 'stars',\n",
       " 'text',\n",
       " 'useful',\n",
       " 'user_id',\n",
       " 'words',\n",
       " 'model']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.460701\n",
      "r2: 0.839328\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|stars|               model|\n",
      "+------------------+-----+--------------------+\n",
      "| 3.978619162700394|  4.0|[0.00131861418102...|\n",
      "|3.7942751679944546|  4.0|[6.01509346811593...|\n",
      "|     4.13363320459|  4.0|[0.00119749398695...|\n",
      "|3.8609924920248204|  4.0|[6.51844831882044...|\n",
      "| 3.784364208530939|  4.0|[0.00146789905875...|\n",
      "| 3.938090791198865|  4.0|[0.00102792119258...|\n",
      "| 2.768740683860572|  2.0|[0.00102986608414...|\n",
      "| 4.627303976338861|  5.0|[0.00147076718646...|\n",
      "| 3.749330720878864|  4.0|[0.00148871509222...|\n",
      "| 4.997540225979817|  5.0|[-7.7833037115245...|\n",
      "| 4.453156920063716|  5.0|[2.88112896669190...|\n",
      "| 4.894827564276035|  5.0|[6.32315336371816...|\n",
      "| 3.298712629100418|  2.0|[9.10666916063386...|\n",
      "|   3.1703814718937|  3.0|[9.29591479785532...|\n",
      "| 4.427272387978683|  5.0|[7.81678155261842...|\n",
      "| 4.726151281977669|  5.0|[0.00150755843857...|\n",
      "| 3.756729222872821|  4.0|[6.10695605504630...|\n",
      "|1.6398778877393632|  1.0|[0.00209529235726...|\n",
      "+------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predict = model.transform(prepared_reviews)\n",
    "train_predict.select(\"prediction\", \"stars\", \"model\").show(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"stars\",metricName=\"r2\")\n",
    "\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(train_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
