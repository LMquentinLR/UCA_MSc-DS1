{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Kafka\n",
    "\n",
    "There exists a neat Python package for communicating with a running Kafka cluster. You can even admin the cluster using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading https://files.pythonhosted.org/packages/75/68/dcb0db055309f680ab2931a3eeb22d865604b638acf8c914bedf4c1a0c8c/kafka_python-2.0.2-py2.py3-none-any.whl (246kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"bootstrap_servers\": \"pkc-4r297.europe-west1.gcp.confluent.cloud:9092\",\n",
    "    \"security_protocol\": \"SASL_SSL\",\n",
    "    \"sasl_mechanism\": \"PLAIN\",\n",
    "    \"sasl_plain_username\": \"{KEY}\",\n",
    "    \"sasl_plain_password\": \"{SECRET}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic=u'main_topic', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "\n",
    "MAIN_TOPIC = \"main_topic\"\n",
    "admin_client = KafkaAdminClient(**params)\n",
    "# with AdminClient you can do anything with Kafka\n",
    "admin_client.delete_topics(admin_client.list_topics())\n",
    "# replication_factor is defined by cluster's configuration\n",
    "# we won't use topic partitioning for this example\n",
    "# thus leaving the number of partitions to 1\n",
    "admin_client.create_topics([\n",
    "    NewTopic(\n",
    "        name=MAIN_TOPIC,\n",
    "        num_partitions=1,\n",
    "        replication_factor=3\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data to Kafka\n",
    "\n",
    "Mind that one can write only `bytes` to Kafka, not strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'main_topic']\n",
      "[u'main_topic']\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "print(admin_client.list_topics())\n",
    "producer = KafkaProducer(**params)\n",
    "for i in range(30):\n",
    "    producer.send(MAIN_TOPIC, bytes(i))\n",
    "print(admin_client.list_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data from Kafka\n",
    "\n",
    "Reading is done with a consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics, Partitions, and Offsets\n",
    "\n",
    "Messages in Kafka are organised into topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'main_topic'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic can have several partitions with different starting and ending offsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{TopicPartition(topic=u'main_topic', partition=0): 0}\n",
      "{TopicPartition(topic=u'main_topic', partition=0): 40}\n"
     ]
    }
   ],
   "source": [
    "from kafka import TopicPartition\n",
    "\n",
    "partitions = [\n",
    "    TopicPartition(MAIN_TOPIC, partition)\n",
    "    for partition in consumer.partitions_for_topic(MAIN_TOPIC)\n",
    "]\n",
    "print(consumer.beginning_offsets(partitions))\n",
    "print(consumer.end_offsets(partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reading something from Kafka, one should assign the consumer to a topic and partition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from a given offset of a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.assign(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can read from any offset of the partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(consumer.position(partitions[0]))\n",
    "consumer.seek(partitions[0], 0)\n",
    "print(consumer.position(partitions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data can be done by batches of any desired size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    data = consumer.poll(\n",
    "        timeout_ms=10,\n",
    "        max_records=1\n",
    "    )[partitions[0]][0].value\n",
    "    print(data)\n",
    "print(consumer.position(partitions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-624d5fa0-ea66-4164-8e60-0f3a0eb061b6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;0.10.0.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.2.6 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 506ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tnet.jpountz.lz4#lz4;1.3.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.10.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.2.6 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-624d5fa0-ea66-4164-8e60-0f3a0eb061b6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/11ms)\n",
      "\u001b]0;IPython: /\u000720/12/10 14:28:44 INFO org.spark_project.jetty.util.log: Logging initialized @4852ms\n",
      "20/12/10 14:28:44 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/12/10 14:28:44 INFO org.spark_project.jetty.server.Server: Started @4961ms\n",
      "20/12/10 14:28:44 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/12/10 14:28:44 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@73750ab7{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "20/12/10 14:28:44 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "20/12/10 14:28:45 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-4395-m/10.132.0.11:8032\n",
      "20/12/10 14:28:45 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-4395-m/10.132.0.11:10200\n",
      "20/12/10 14:28:47 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar added multiple times to distributed cache.\n",
      "20/12/10 14:28:47 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar added multiple times to distributed cache.\n",
      "20/12/10 14:28:47 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "20/12/10 14:28:47 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar added multiple times to distributed cache.\n",
      "20/12/10 14:28:47 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar added multiple times to distributed cache.\n",
      "20/12/10 14:28:47 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n",
      "20/12/10 14:28:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1607606182296_0002\n",
      "20/12/10 14:28:53 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: \n",
      "\tmetric.reporters = []\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tpartition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treconnect.backoff.ms = 50\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tbootstrap.servers = [pkc-4r297.europe-west1.gcp.confluent.cloud:9092]\n",
      "\tssl.keystore.type = JKS\n",
      "\tenable.auto.commit = false\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tinterceptor.classes = null\n",
      "\texclude.internal.topics = true\n",
      "\tssl.truststore.password = null\n",
      "\tclient.id = \n",
      "\tssl.endpoint.identification.algorithm = null\n",
      "\tmax.poll.records = 1\n",
      "\tcheck.crcs = true\n",
      "\trequest.timeout.ms = 40000\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\tssl.truststore.type = JKS\n",
      "\tssl.truststore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tfetch.min.bytes = 1\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tgroup.id = spark-kafka-source-815a55a9-bea3-435a-9b94-894d022510e8--78271051-driver-0\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.key.password = null\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tsession.timeout.ms = 30000\n",
      "\tmetrics.num.samples = 2\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tssl.protocol = TLS\n",
      "\tssl.provider = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n",
      "\tssl.keystore.location = null\n",
      "\tssl.cipher.suites = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tauto.offset.reset = earliest\n",
      "\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: \n",
      "\tmetric.reporters = []\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tpartition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treconnect.backoff.ms = 50\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tbootstrap.servers = [pkc-4r297.europe-west1.gcp.confluent.cloud:9092]\n",
      "\tssl.keystore.type = JKS\n",
      "\tenable.auto.commit = false\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tinterceptor.classes = null\n",
      "\texclude.internal.topics = true\n",
      "\tssl.truststore.password = null\n",
      "\tclient.id = consumer-1\n",
      "\tssl.endpoint.identification.algorithm = null\n",
      "\tmax.poll.records = 1\n",
      "\tcheck.crcs = true\n",
      "\trequest.timeout.ms = 40000\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\tssl.truststore.type = JKS\n",
      "\tssl.truststore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tfetch.min.bytes = 1\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tgroup.id = spark-kafka-source-815a55a9-bea3-435a-9b94-894d022510e8--78271051-driver-0\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.key.password = null\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tsession.timeout.ms = 30000\n",
      "\tmetrics.num.samples = 2\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tssl.protocol = TLS\n",
      "\tssl.provider = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n",
      "\tssl.keystore.location = null\n",
      "\tssl.cipher.suites = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tauto.offset.reset = earliest\n",
      "\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version : 0.10.0.1\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: \n",
      "\tmetric.reporters = []\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tpartition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treconnect.backoff.ms = 50\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tbootstrap.servers = [pkc-4r297.europe-west1.gcp.confluent.cloud:9092]\n",
      "\tssl.keystore.type = JKS\n",
      "\tenable.auto.commit = false\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tinterceptor.classes = null\n",
      "\texclude.internal.topics = true\n",
      "\tssl.truststore.password = null\n",
      "\tclient.id = \n",
      "\tssl.endpoint.identification.algorithm = null\n",
      "\tmax.poll.records = 1\n",
      "\tcheck.crcs = true\n",
      "\trequest.timeout.ms = 40000\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\tssl.truststore.type = JKS\n",
      "\tssl.truststore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tfetch.min.bytes = 1\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tgroup.id = spark-kafka-source-d8d1dc5b-9e91-424d-918f-5f0064ddc854--223180200-driver-0\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.key.password = null\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tsession.timeout.ms = 30000\n",
      "\tmetrics.num.samples = 2\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tssl.protocol = TLS\n",
      "\tssl.provider = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n",
      "\tssl.keystore.location = null\n",
      "\tssl.cipher.suites = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tauto.offset.reset = earliest\n",
      "\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: \n",
      "\tmetric.reporters = []\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tpartition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treconnect.backoff.ms = 50\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tbootstrap.servers = [pkc-4r297.europe-west1.gcp.confluent.cloud:9092]\n",
      "\tssl.keystore.type = JKS\n",
      "\tenable.auto.commit = false\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tinterceptor.classes = null\n",
      "\texclude.internal.topics = true\n",
      "\tssl.truststore.password = null\n",
      "\tclient.id = consumer-2\n",
      "\tssl.endpoint.identification.algorithm = null\n",
      "\tmax.poll.records = 1\n",
      "\tcheck.crcs = true\n",
      "\trequest.timeout.ms = 40000\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\tssl.truststore.type = JKS\n",
      "\tssl.truststore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tfetch.min.bytes = 1\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tgroup.id = spark-kafka-source-d8d1dc5b-9e91-424d-918f-5f0064ddc854--223180200-driver-0\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.key.password = null\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tsession.timeout.ms = 30000\n",
      "\tmetrics.num.samples = 2\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tssl.protocol = TLS\n",
      "\tssl.provider = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n",
      "\tssl.keystore.location = null\n",
      "\tssl.cipher.suites = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tauto.offset.reset = earliest\n",
      "\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version : 0.10.0.1\n",
      "20/12/10 14:28:54 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/12/10 14:28:55 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:55 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:56 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:56 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:56 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: \n",
      "\tmetric.reporters = []\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tpartition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treconnect.backoff.ms = 50\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tbootstrap.servers = [pkc-4r297.europe-west1.gcp.confluent.cloud:9092]\n",
      "\tssl.keystore.type = JKS\n",
      "\tenable.auto.commit = false\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tinterceptor.classes = null\n",
      "\texclude.internal.topics = true\n",
      "\tssl.truststore.password = null\n",
      "\tclient.id = \n",
      "\tssl.endpoint.identification.algorithm = null\n",
      "\tmax.poll.records = 1\n",
      "\tcheck.crcs = true\n",
      "\trequest.timeout.ms = 40000\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\tssl.truststore.type = JKS\n",
      "\tssl.truststore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tfetch.min.bytes = 1\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tgroup.id = spark-kafka-source-209f30f3-9c35-4df7-955d-e31cb0b11946--272532385-driver-0\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.key.password = null\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tsession.timeout.ms = 30000\n",
      "\tmetrics.num.samples = 2\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tssl.protocol = TLS\n",
      "\tssl.provider = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n",
      "\tssl.keystore.location = null\n",
      "\tssl.cipher.suites = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tauto.offset.reset = earliest\n",
      "\n",
      "20/12/10 14:28:56 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: \n",
      "\tmetric.reporters = []\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tpartition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treconnect.backoff.ms = 50\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tbootstrap.servers = [pkc-4r297.europe-west1.gcp.confluent.cloud:9092]\n",
      "\tssl.keystore.type = JKS\n",
      "\tenable.auto.commit = false\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tinterceptor.classes = null\n",
      "\texclude.internal.topics = true\n",
      "\tssl.truststore.password = null\n",
      "\tclient.id = consumer-3\n",
      "\tssl.endpoint.identification.algorithm = null\n",
      "\tmax.poll.records = 1\n",
      "\tcheck.crcs = true\n",
      "\trequest.timeout.ms = 40000\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\tssl.truststore.type = JKS\n",
      "\tssl.truststore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tfetch.min.bytes = 1\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tgroup.id = spark-kafka-source-209f30f3-9c35-4df7-955d-e31cb0b11946--272532385-driver-0\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.key.password = null\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tsession.timeout.ms = 30000\n",
      "\tmetrics.num.samples = 2\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tssl.protocol = TLS\n",
      "\tssl.provider = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n",
      "\tssl.keystore.location = null\n",
      "\tssl.cipher.suites = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tauto.offset.reset = earliest\n",
      "\n",
      "20/12/10 14:28:56 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version : 0.10.0.1\n",
      "20/12/10 14:28:56 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId : a7a17cdec9eaa6c5\n",
      "20/12/10 14:28:57 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:57 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:57 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:57 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:57 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:58 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:58 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:58 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:58 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:59 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:59 WARN org.apache.kafka.clients.NetworkClient: Bootstrap broker pkc-4r297.europe-west1.gcp.confluent.cloud:9092 disconnected\n",
      "20/12/10 14:28:59 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@73750ab7{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.4 /home/user/spark-and-kafka.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "kafka-examples.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
