{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Optimization - TD1 - Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.8.0\n",
      "11.1\n",
      "8005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local GPU Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 10 22:26:11 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 980 Ti  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 32%   38C    P5    34W / 260W |    886MiB /  6075MiB |     23%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       933      G   /usr/lib/xorg/Xorg                129MiB |\r\n",
      "|    0   N/A  N/A      3227      G   /usr/lib/xorg/Xorg                541MiB |\r\n",
      "|    0   N/A  N/A      3360      G   /usr/bin/gnome-shell               48MiB |\r\n",
      "|    0   N/A  N/A      4149      G   ...AAAAAAAA== --shared-files      104MiB |\r\n",
      "|    0   N/A  N/A      5128      G   ...AAAAAAAAA= --shared-files       12MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "#Activate GPU usage, Runtime -> Change Runtime Type -> Choose GPU type\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 10 22:26:13 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 980 Ti  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 32%   38C    P5    34W / 260W |   1509MiB /  6075MiB |     39%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       933      G   /usr/lib/xorg/Xorg                129MiB |\r\n",
      "|    0   N/A  N/A      3227      G   /usr/lib/xorg/Xorg                541MiB |\r\n",
      "|    0   N/A  N/A      3360      G   /usr/bin/gnome-shell               48MiB |\r\n",
      "|    0   N/A  N/A      4149      G   ...AAAAAAAA== --shared-files      104MiB |\r\n",
      "|    0   N/A  N/A      5128      G   ...AAAAAAAAA= --shared-files       12MiB |\r\n",
      "|    0   N/A  N/A     49585      C   .../qlr/anaconda3/bin/python      622MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(10000000,device='cuda')\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 10 22:26:13 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 980 Ti  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 32%   38C    P5    34W / 260W |   1469MiB /  6075MiB |     31%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       933      G   /usr/lib/xorg/Xorg                129MiB |\r\n",
      "|    0   N/A  N/A      3227      G   /usr/lib/xorg/Xorg                541MiB |\r\n",
      "|    0   N/A  N/A      3360      G   /usr/bin/gnome-shell               48MiB |\r\n",
      "|    0   N/A  N/A      4149      G   ...AAAAAAAA== --shared-files      104MiB |\r\n",
      "|    0   N/A  N/A      5128      G   ...AAAAAAAAA= --shared-files       12MiB |\r\n",
      "|    0   N/A  N/A     49585      C   .../qlr/anaconda3/bin/python      582MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "del a \n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello from process 3 (out of 4)!\n",
      "Hello from process 2 (out of 4)!\n",
      "\n",
      "Hello from process 0 (out of 4)!\n",
      "Hello from process 1 (out of 4)!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "def print_rank():\n",
    "    print('\\nHello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\n",
    "\n",
    "def init_process(rank, size, fn, backend='gloo'): #gloo: anyone can send messages to anyone\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29501'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn()\n",
    "\n",
    "def main(fn, size=4):\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, fn))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "main(print_rank, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Which method is used to launch multiple processes?\n",
    "\n",
    "The method ``init_process`` is used.\n",
    "\n",
    "#### Q2: After initilization, the rank of the process and the worldsize can be obtained by which functions in torch.distributed?\n",
    "\n",
    "The methods of a Process object used for this purpose are ``get_rank()`` and ``get_world_size()``.\n",
    "\n",
    "## Communication\n",
    "\n",
    "### Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am 2 of 4 with a tensor 2\n",
      "\n",
      "I am 3 of 4 with a tensor 3\n",
      "I am 0 of 4 with a tensor 0\n",
      "I am 1 of 4 with a tensor 1\n",
      "\n",
      "**********\n",
      "Starting Communication\n",
      "************\n",
      "\n",
      "\n",
      "Rank 2 has data 0\n",
      "Rank 0 has data 0\n",
      "\n",
      "Rank 1 has data 0\n",
      "\n",
      "Rank 3 has data 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def broadcast():\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    tensor = torch.tensor(rank)\n",
    "    group = dist.new_group([0,1,2,3])\n",
    "    print(f\"\\nI am {rank} of {size} with a tensor {tensor}\")\n",
    "    \n",
    "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
    "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
    "    print(f\"\\nRank {rank} has data {tensor}\")\n",
    "\n",
    "main(broadcast, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: In the above code, which rank is the one who broadcasts?\n",
    "\n",
    "The rank that broadcast to the other is ```rank 0```.\n",
    "\n",
    "#### Task 1: If Rank 0 just wants to broadcast to a random subset of all the processes, please write down the new code to acheive that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset: [0, 1, 2, 3]\n",
      "\n",
      "I am 3 of 4 with a tensor 3\n",
      "\n",
      "I am 0 of 4 with a tensor 0\n",
      "\n",
      "I am 2 of 4 with a tensor 2\n",
      "I am 1 of 4 with a tensor 1\n",
      "**********\n",
      "Starting Communication\n",
      "************\n",
      "\n",
      "\n",
      "Rank 2 has data 0\n",
      "Rank 0 has data 0\n",
      "Rank 3 has data 0\n",
      "\n",
      "Rank 1 has data 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "lst = [0]+sorted(rd.sample([1,2,3], rd.randint(0,3)))\n",
    "print(f\"subset: {lst}\")\n",
    "\n",
    "def broadcast_random():\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    tensor = torch.tensor(rank)\n",
    "    group =  dist.new_group(lst)\n",
    "    print(f\"\\nI am {rank} of {size} with a tensor {tensor}\")\n",
    "    \n",
    "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
    "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
    "    print(f\"\\nRank {rank} has data {tensor}\")\n",
    "\n",
    "main(broadcast_random, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am 3 of 4 with a tensor 4\n",
      "\n",
      "I am 0 of 4 with a tensor 1\n",
      "**********\n",
      "Starting Communication\n",
      "************\n",
      "I am 2 of 4 with a tensor 3\n",
      "I am 1 of 4 with a tensor 2\n",
      "\n",
      "\n",
      "\n",
      "Rank 2 has data 7\n",
      "Rank 3 has data 4\n",
      "\n",
      "\n",
      "Rank 1 has data 9\n",
      "Rank 0 has data 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reduce():\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    tensor = torch.tensor(rank+1)\n",
    "    if rank == 0:\n",
    "        tensor_old = tensor.clone()\n",
    "    group = dist.new_group([0,1,2,3])\n",
    "    print(f\"\\nI am {rank} of {size} with a tensor {tensor}\")\n",
    "    if rank == 0:\n",
    "        print(\"**********\\nStarting Communication\\n************\")\n",
    "    dist.reduce(tensor=tensor, dst=0, op=dist.ReduceOp.SUM, group = group)\n",
    "    if rank == 0:\n",
    "        tensor -= tensor_old\n",
    "    print(f\"\\nRank {rank} has data {tensor.item()}\")\n",
    "\n",
    "main(reduce, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: What does the above code achieve?\n",
    "\n",
    "The code above achieve a reduce operation over the tensors held in a set of nodes. Here the reducer function performs a sum with the implicit accumulator set to 0\n",
    "\n",
    "#### Q5: Check the values of every rank after \"reduce\", try to explain the reason.**\n",
    "\n",
    "The operation performed by the reduce is similar to the following code, albeit in a parallelized way:\n",
    "\n",
    "```python\n",
    "from functools import reduce\n",
    "reduce(lambda x, y: x+y, map(lambda x:x+1 if x !=0 else x, [0,1,2,3]))\n",
    "````\n",
    "\n",
    "The reduce operation is triggered when the main function is evaluated for rank=0, which evaluate all other node and perform a sum of the value held in their tensor (the reduce operation), leading to:\n",
    "\n",
    "```tensor of rank 0 is updated with value (1+1) + (2+1) + (3+1) = 9```\n",
    "\n",
    "#### Task 2 [Server-Client communication]: Write a function which runs for 10 iterations: Among each iteration,\n",
    "\n",
    "1. rank 0 broadcasts to a random subset of all the processes\n",
    "2. the processes in the subset update their states by adding one unit\n",
    "3. rank 0 gets the average of the states from the processes in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reduce_10_iterations():\n",
    "    \"\"\"\n",
    "    Performs 10 reduction operations\n",
    "    \"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    tensor = torch.tensor(rank+1)\n",
    "    group = dist.new_group([0,1,2,3])\n",
    "    \n",
    "    print(f\"\\nI am {rank+1} of {size} with a tensor {tensor}\")\n",
    "    \n",
    "    # Records old\n",
    "    if rank == 0: \n",
    "        print(\"****\\nStarting Communication\\n****\")\n",
    "        tensor_old = tensor.clone()\n",
    "    \n",
    "    # Performs the 10-fold reduction\n",
    "    for i in range(10):\n",
    "        if rank == 0:\n",
    "            print(f\"\\n#### ITERATION/EVALUATION {i}\")\n",
    "        dist.reduce(tensor=tensor, dst=0, op=dist.ReduceOp.SUM, group = group)\n",
    "    \n",
    "    # Removes over-added value\n",
    "    if rank == 0: \n",
    "        tensor -= tensor_old\n",
    "    \n",
    "    # Prints result\n",
    "    print(f\"\\nRank {rank+1} has data {tensor.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am 2 of 4 with a tensor 2\n",
      "I am 3 of 4 with a tensor 3\n",
      "I am 4 of 4 with a tensor 4\n",
      "\n",
      "\n",
      "\n",
      "I am 1 of 4 with a tensor 1\n",
      "****\n",
      "Starting Communication\n",
      "****\n",
      "\n",
      "#### ITERATION/EVALUATION 0\n",
      "\n",
      "#### ITERATION/EVALUATION 1\n",
      "\n",
      "#### ITERATION/EVALUATION 2\n",
      "\n",
      "#### ITERATION/EVALUATION 3\n",
      "\n",
      "#### ITERATION/EVALUATION 4\n",
      "\n",
      "#### ITERATION/EVALUATION 5\n",
      "\n",
      "#### ITERATION/EVALUATION 6\n",
      "\n",
      "#### ITERATION/EVALUATION 7\n",
      "\n",
      "Rank 4 has data 4\n",
      "#### ITERATION/EVALUATION 8\n",
      "\n",
      "\n",
      "Rank 3 has data 43\n",
      "\n",
      "#### ITERATION/EVALUATION 9\n",
      "\n",
      "Rank 2 has data 252\n",
      "\n",
      "Rank 1 has data 1065\n"
     ]
    }
   ],
   "source": [
    "main(reduce_10_iterations, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am 2 of 4 with a tensor 3\n",
      "I am 3 of 4 with a tensor 4\n",
      "I am 1 of 4 with a tensor 2\n",
      "I am 0 of 4 with a tensor 1\n",
      "\n",
      "\n",
      "\n",
      "Rank 2 has data 4\n",
      "Rank 3 has data 4\n",
      "****\n",
      "Starting Communication\n",
      "****\n",
      "\n",
      "\n",
      "\n",
      "Rank 0 has data 2\n",
      "\n",
      "Rank 1 has data 2\n"
     ]
    }
   ],
   "source": [
    "def send_receive():\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    tensor = torch.tensor(rank+1)\n",
    "    print(f\"\\nI am {rank} of {size} with a tensor {tensor}\")\n",
    "    if rank == 0:\n",
    "        print(\"****\\nStarting Communication\\n****\")\n",
    "        dist.recv(tensor, src=1)\n",
    "    if rank == 1:\n",
    "        dist.send(tensor, dst=0)\n",
    "    if rank == 2:\n",
    "        dist.recv(tensor)\n",
    "    if rank == 3:\n",
    "        dist.send(tensor, dst=2)\n",
    "    print(f\"\\nRank {rank} has data {tensor.item()}\")\n",
    "\n",
    "main(send_receive, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6: What does the above code acheive?\n",
    "\n",
    "Node w/ Rank 0 receives data from Node w/ Rank 1. Node w/ Rank 2 receives data from Node w/ Rank 4.\n",
    "\n",
    "#### Task 3: Use \"send\" and \"receive\" to achieve the goal of reduce that rank 0 has the sum of all the values from rank 1 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am 2 of 4 with a tensor 2\n",
      "I am 3 of 4 with a tensor 3\n",
      "I am 4 of 4 with a tensor 4\n",
      "\n",
      "\n",
      "\n",
      "I am 1 of 4 with a tensor 1\n",
      "\n",
      "****\n",
      "Starting Communication\n",
      "****\n",
      "\n",
      "Rank 4 has data 4\n",
      "\n",
      "Rank 2 has data 9\n",
      "Rank 3 has data 7\n",
      "\n",
      "\n",
      "Rank 1 has data 9\n"
     ]
    }
   ],
   "source": [
    "def send_receive_reduce():\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    tensor = torch.tensor(rank+1)\n",
    "    t = tensor.clone()\n",
    "    \n",
    "    print(f\"\\nI am {rank+1} of {size} with a tensor {tensor}\")\n",
    "    if rank == 0:\n",
    "        print(\"\\n****\\nStarting Communication\\n****\")\n",
    "        dist.recv(tensor, src=1)\n",
    "    if rank == 1:\n",
    "        dist.recv(tensor, src=2)\n",
    "        tensor += t\n",
    "        dist.send(tensor, dst=0)\n",
    "    if rank == 2:\n",
    "        dist.recv(tensor, src=3)\n",
    "        tensor += t\n",
    "        dist.send(tensor, dst=1)\n",
    "    if rank == 3:\n",
    "        dist.send(tensor, dst=2)\n",
    "    print(f\"\\nRank {rank+1} has data {tensor.item()}\")\n",
    "    \n",
    "main(send_receive_reduce, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Spawn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Spawn.py\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as multiprocess\n",
    "import argparse\n",
    "\n",
    "\n",
    "def parse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--size', type=int, help='the number of processes')\n",
    "    parser.add_argument('--func', type=str, help='choose the function to execute')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def print_rank(tensor):\n",
    "    print('Hello from process {} (out of {}) with tensor {}!'.format(dist.get_rank(), dist.get_world_size(), tensor))\n",
    "\n",
    "def broadcast(tensor):\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    group = dist.new_group([0,1])\n",
    "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
    "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor)\n",
    "\n",
    "def init_process(rank, size, fn, backend):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29501'\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = torch.device('cuda:'+str(rank))\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    tensor = torch.tensor(rank, device=device)\n",
    "    # Use torch.Tensor.item() to get a Python number from a tensor containing a single value:\n",
    "    print(f\"I am {rank} with a tensor {tensor.item()}\")\n",
    "    # Get a numpy array from a tensor array: tensor.numpy() or tensor.cpu().numpy()\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(tensor)\n",
    "\n",
    "if __name__== '__main__':\n",
    "    args = parse()\n",
    "    if torch.cuda.is_available() == True:\n",
    "        backend = 'nccl'\n",
    "        if torch.cuda.device_count()<args.size:\n",
    "            raise ValueError('size should not larger than the number of GPUs')\n",
    "    else:\n",
    "        backend = 'gloo'\n",
    "            \n",
    "    print(f\"Backend: {backend}\")\n",
    "    function_mapping = {'print_rank':print_rank, 'broadcast':broadcast}\n",
    "    multiprocess.spawn(init_process, args=(args.size, function_mapping[args.func], backend), nprocs=args.size,join=True, daemon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: nccl\n",
      "I am 0 with a tensor 0\n",
      "Hello from process 0 (out of 1) with tensor 0!\n"
     ]
    }
   ],
   "source": [
    "! python Spawn.py --size 1 --func \"print_rank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: How to put the tensor to GPU?\n",
    "\n",
    "```X=torch.ones(10)``` or ```x.device``` then ```x.to(\"cuda\")```\n",
    "\n",
    "#### Q8: How to use \"spawn\" funtion offered by multiprocessing package?\n",
    "\n",
    "Spawn is a script to be launch via command line that reuses the functions already seen before.\n",
    "\n",
    "#### Task 4 [After class]: Reserve two GPUs from NEF and change the script that rank 1 can broadcast to the rest. Then rerun to check.\n",
    "\n",
    "see screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
